{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet自编码器详解 - 从零理解每一层\n",
    "\n",
    "本notebook将详细解析ResNet自编码器的结构，特别关注**预处理卷积层**的重要作用。\n",
    "\n",
    "## 目标\n",
    "- 理解为什么需要预处理卷积层\n",
    "- 观察每层的输出形状变化\n",
    "- 掌握ResNet残差块的工作原理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch版本: 2.5.1\n",
      "CUDA可用: True\n"
     ]
    }
   ],
   "source": [
    "# 导入必要的库\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "print(f\"PyTorch版本: {torch.__version__}\")\n",
    "print(f\"CUDA可用: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 残差块的基本结构\n",
    "\n",
    "首先我们理解残差块，这是ResNet的核心组件："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 残差块定义完成\n"
     ]
    }
   ],
   "source": [
    "class Residual_block(nn.Module):\n",
    "    \"\"\"ResNet残差块\n",
    "    \n",
    "    Args:\n",
    "        ic: 输入通道数 (input channels)\n",
    "        oc: 输出通道数 (output channels) \n",
    "        stride: 步长，控制尺寸缩放\n",
    "    \"\"\"\n",
    "    def __init__(self, ic, oc, stride=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 主路径：两个3x3卷积\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(ic, oc, stride=stride, padding=1, kernel_size=3),\n",
    "            nn.BatchNorm2d(oc),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(oc, oc, stride=1, padding=1, kernel_size=3),\n",
    "            nn.BatchNorm2d(oc)\n",
    "        )\n",
    "        \n",
    "        # 跳跃连接：当输入输出维度不匹配时需要调整\n",
    "        self.downsample = None \n",
    "        if (stride != 1) or (ic != oc): \n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv2d(ic, oc, stride=stride, kernel_size=1),  # 1x1卷积调整维度\n",
    "                nn.BatchNorm2d(oc)\n",
    "            )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        # 保存输入作为残差连接\n",
    "        residual = x\n",
    "        \n",
    "        # 主路径前向传播\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        \n",
    "        # 如果需要，调整残差的维度\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(residual)\n",
    "        \n",
    "        # 残差连接：F(x) + x\n",
    "        out = out + residual\n",
    "        out = F.relu(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "print(\"✅ 残差块定义完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 测试残差块 - 理解维度变化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试输入形状: torch.Size([2, 3, 64, 64])\n",
      "含义: (2张图像, 3通道RGB, 64x64像素)\n",
      "\n",
      "=== 测试1: 通道数不变，尺寸不变 ===\n",
      "输入: torch.Size([2, 3, 64, 64]) → 输出: torch.Size([2, 3, 64, 64])\n",
      "结果: 只处理特征，不改变维度\n",
      "\n",
      "=== 测试2: 增加通道数，尺寸不变 ===\n",
      "输入: torch.Size([2, 3, 64, 64]) → 输出: torch.Size([2, 32, 64, 64])\n",
      "结果: 通道数 3→32，特征更丰富了！\n",
      "\n",
      "=== 测试3: 增加通道数，缩小尺寸 ===\n",
      "输入: torch.Size([2, 3, 64, 64]) → 输出: torch.Size([2, 64, 32, 32])\n",
      "结果: 通道数 3→64，尺寸 64→32，信息更集中！\n"
     ]
    }
   ],
   "source": [
    "# 创建测试数据：模拟一个batch的RGB图像\n",
    "batch_size = 2\n",
    "test_input = torch.randn(batch_size, 3, 64, 64)  # (N, C, H, W)\n",
    "print(f\"测试输入形状: {test_input.shape}\")\n",
    "print(f\"含义: ({batch_size}张图像, 3通道RGB, 64x64像素)\\n\")\n",
    "\n",
    "# 测试1: 保持通道数和尺寸不变的残差块\n",
    "print(\"=== 测试1: 通道数不变，尺寸不变 ===\")\n",
    "block1 = Residual_block(ic=3, oc=3, stride=1)\n",
    "output1 = block1(test_input)\n",
    "print(f\"输入: {test_input.shape} → 输出: {output1.shape}\")\n",
    "print(\"结果: 只处理特征，不改变维度\\n\")\n",
    "\n",
    "# 测试2: 增加通道数，保持尺寸\n",
    "print(\"=== 测试2: 增加通道数，尺寸不变 ===\")\n",
    "block2 = Residual_block(ic=3, oc=32, stride=1)\n",
    "output2 = block2(test_input)\n",
    "print(f\"输入: {test_input.shape} → 输出: {output2.shape}\")\n",
    "print(\"结果: 通道数 3→32，特征更丰富了！\\n\")\n",
    "\n",
    "# 测试3: 增加通道数，缩小尺寸\n",
    "print(\"=== 测试3: 增加通道数，缩小尺寸 ===\")\n",
    "block3 = Residual_block(ic=3, oc=64, stride=2)\n",
    "output3 = block3(test_input)\n",
    "print(f\"输入: {test_input.shape} → 输出: {output3.shape}\")\n",
    "print(\"结果: 通道数 3→64，尺寸 64→32，信息更集中！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 为什么需要预处理卷积层？\n",
    "\n",
    "现在我们来理解预处理层的重要性："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤔 问题：为什么不能直接把3通道图像输入残差网络？\n",
      "\n",
      "❌ 方案1: 直接用3通道 → 残差块\n",
      "   输入: torch.Size([2, 3, 64, 64]) → 输出: torch.Size([2, 128, 32, 32])\n",
      "   ⚠️  可以工作，但跨度太大，训练困难\n",
      "\n",
      "✅ 方案2: 预处理层 → 残差块\n",
      "   步骤1 - 预处理: torch.Size([2, 3, 64, 64]) → torch.Size([2, 32, 64, 64])\n",
      "   步骤2 - 残差块: torch.Size([2, 32, 64, 64]) → torch.Size([2, 128, 32, 32])\n",
      "   ✨ 平滑过渡: 3→32→128，更容易训练！\n"
     ]
    }
   ],
   "source": [
    "print(\"🤔 问题：为什么不能直接把3通道图像输入残差网络？\\n\")\n",
    "\n",
    "# 模拟没有预处理的情况\n",
    "print(\"❌ 方案1: 直接用3通道 → 残差块\")\n",
    "try:\n",
    "    # 假设我们想直接从3通道跳到128通道\n",
    "    direct_block = Residual_block(ic=3, oc=128, stride=2)\n",
    "    direct_output = direct_block(test_input)\n",
    "    print(f\"   输入: {test_input.shape} → 输出: {direct_output.shape}\")\n",
    "    print(\"   ⚠️  可以工作，但跨度太大，训练困难\\n\")\n",
    "except Exception as e:\n",
    "    print(f\"   ❌ 失败: {e}\\n\")\n",
    "\n",
    "print(\"✅ 方案2: 预处理层 → 残差块\")\n",
    "# 定义预处理层\n",
    "preconv = nn.Sequential(\n",
    "    nn.Conv2d(3, 32, kernel_size=3, padding=1, stride=1, bias=False),\n",
    "    nn.BatchNorm2d(32),\n",
    "    nn.ReLU(inplace=True)\n",
    ")\n",
    "\n",
    "# 先预处理，再用残差块\n",
    "preprocessed = preconv(test_input)\n",
    "smooth_block = Residual_block(ic=32, oc=128, stride=2)\n",
    "smooth_output = smooth_block(preprocessed)\n",
    "\n",
    "print(f\"   步骤1 - 预处理: {test_input.shape} → {preprocessed.shape}\")\n",
    "print(f\"   步骤2 - 残差块: {preprocessed.shape} → {smooth_output.shape}\")\n",
    "print(\"   ✨ 平滑过渡: 3→32→128，更容易训练！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 完整的ResNet编码器\n",
    "\n",
    "现在我们构建完整的ResNet编码器，观察每层的变化："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetEncoder(nn.Module):\n",
    "    \"\"\"改进的ResNet编码器 - 处理batch_size=1的情况\"\"\"\n",
    "    \n",
    "    def __init__(self, block=Residual_block, num_layers=[2, 1, 1, 1]):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 预处理卷积层\n",
    "        self.preconv = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # 构建残差层组的辅助函数\n",
    "        def make_residual_group(block, ic, oc, num_layer, stride=1):\n",
    "            layers = []\n",
    "            layers.append(block(ic, oc, stride))\n",
    "            for i in range(num_layer - 1):\n",
    "                layers.append(block(oc, oc))\n",
    "            return nn.Sequential(*layers)\n",
    "\n",
    "        # 4个残差层组\n",
    "        self.layer0 = make_residual_group(block, ic=32,  oc=64,  num_layer=num_layers[0], stride=2)\n",
    "        self.layer1 = make_residual_group(block, ic=64,  oc=128, num_layer=num_layers[1], stride=2)\n",
    "        self.layer2 = make_residual_group(block, ic=128, oc=128, num_layer=num_layers[2], stride=2)\n",
    "        self.layer3 = make_residual_group(block, ic=128, oc=64,  num_layer=num_layers[3], stride=2)\n",
    "\n",
    "        # 最终特征处理 - 改进版本\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d((4, 4))  # 确保输出是4x4\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64*4*4, 64),\n",
    "            nn.ReLU(inplace=True)  # 移除BatchNorm1d避免batch_size=1问题\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"前向传播，自动处理batch_size问题\"\"\"\n",
    "        outputs = {}\n",
    "        \n",
    "        # 如果batch_size=1且在训练模式，临时切换到eval模式\n",
    "        training_mode = self.training\n",
    "        if x.size(0) == 1 and training_mode:\n",
    "            self.eval()\n",
    "        \n",
    "        try:\n",
    "            outputs['input'] = x\n",
    "            \n",
    "            x = self.preconv(x)\n",
    "            outputs['preconv'] = x\n",
    "            \n",
    "            x = self.layer0(x)\n",
    "            outputs['layer0'] = x\n",
    "            \n",
    "            x = self.layer1(x)\n",
    "            outputs['layer1'] = x\n",
    "            \n",
    "            x = self.layer2(x)\n",
    "            outputs['layer2'] = x\n",
    "            \n",
    "            x = self.layer3(x)\n",
    "            outputs['layer3'] = x\n",
    "            \n",
    "            x = self.global_pool(x)  # 确保空间维度\n",
    "            x = self.fc(x)\n",
    "            outputs['final'] = x\n",
    "            \n",
    "        finally:\n",
    "            # 恢复原来的训练模式\n",
    "            if training_mode:\n",
    "                self.train()\n",
    "        \n",
    "        return x, outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 层次结构分析 - 观察信息流动"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 ResNet编码器的层次结构分析\n",
      "\n",
      "================================================================================\n",
      "input      | 形状: torch.Size([1, 3, 64, 64]) | 原始RGB图像\n",
      "           | →   3个通道, 64×64像素, 总计 12288个特征\n",
      "--------------------------------------------------------------------------------\n",
      "preconv    | 形状: torch.Size([1, 32, 64, 64]) | 预处理后的特征图\n",
      "           | →  32个通道, 64×64像素, 总计131072个特征\n",
      "--------------------------------------------------------------------------------\n",
      "layer0     | 形状: torch.Size([1, 64, 32, 32]) | 第1组残差块 - 检测基本特征\n",
      "           | →  64个通道, 32×32像素, 总计 65536个特征\n",
      "--------------------------------------------------------------------------------\n",
      "layer1     | 形状: torch.Size([1, 128, 16, 16]) | 第2组残差块 - 检测复合特征\n",
      "           | → 128个通道, 16×16像素, 总计 32768个特征\n",
      "--------------------------------------------------------------------------------\n",
      "layer2     | 形状: torch.Size([1, 128, 8, 8]) | 第3组残差块 - 检测高级特征\n",
      "           | → 128个通道,  8× 8像素, 总计  8192个特征\n",
      "--------------------------------------------------------------------------------\n",
      "layer3     | 形状: torch.Size([1, 64, 4, 4]) | 第4组残差块 - 最抽象特征\n",
      "           | →  64个通道,  4× 4像素, 总计  1024个特征\n",
      "--------------------------------------------------------------------------------\n",
      "final      | 形状: torch.Size([1, 64]) | 最终编码向量\n",
      "           | → 64维特征向量\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "🎯 关键观察点:\n",
      "1. 📈 通道数逐渐增加: 3 → 32 → 64 → 128 → 128 → 64\n",
      "2. 📉 空间尺寸逐渐减小: 64×64 → 64×64 → 32×32 → 16×16 → 8×8 → 4×4\n",
      "3. 🧠 信息密度逐渐增大: 从像素级特征到抽象语义特征\n",
      "4. 🎯 最终得到64维的紧凑表示，包含了图像的核心信息\n"
     ]
    }
   ],
   "source": [
    "# 创建编码器实例\n",
    "encoder = ResNetEncoder()\n",
    "\n",
    "# 使用更真实的图像尺寸\n",
    "sample_image = torch.randn(1, 3, 64, 64)  # 1张64x64的RGB图像\n",
    "\n",
    "# 前向传播并获取所有中间结果\n",
    "final_encoding, layer_outputs = encoder(sample_image)\n",
    "\n",
    "print(\"🔍 ResNet编码器的层次结构分析\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 分析每层的变化\n",
    "layer_info = [\n",
    "    ('input',    '原始RGB图像'),\n",
    "    ('preconv',  '预处理后的特征图'),\n",
    "    ('layer0',   '第1组残差块 - 检测基本特征'),\n",
    "    ('layer1',   '第2组残差块 - 检测复合特征'),\n",
    "    ('layer2',   '第3组残差块 - 检测高级特征'),\n",
    "    ('layer3',   '第4组残差块 - 最抽象特征'),\n",
    "    ('final',    '最终编码向量')\n",
    "]\n",
    "\n",
    "for layer_name, description in layer_info:\n",
    "    if layer_name in layer_outputs:\n",
    "        shape = layer_outputs[layer_name].shape\n",
    "        \n",
    "        if len(shape) == 4:  # 图像数据 (N, C, H, W)\n",
    "            n, c, h, w = shape\n",
    "            total_elements = c * h * w\n",
    "            print(f\"{layer_name:10} | 形状: {shape} | {description}\")\n",
    "            print(f\"{'':10} | → {c:3d}个通道, {h:2d}×{w:2d}像素, 总计{total_elements:6d}个特征\")\n",
    "        else:  # 向量数据 (N, Features)\n",
    "            n, features = shape\n",
    "            print(f\"{layer_name:10} | 形状: {shape} | {description}\")\n",
    "            print(f\"{'':10} | → {features}维特征向量\")\n",
    "        \n",
    "        print(\"-\" * 80)\n",
    "\n",
    "print(\"\\n🎯 关键观察点:\")\n",
    "print(\"1. 📈 通道数逐渐增加: 3 → 32 → 64 → 128 → 128 → 64\")\n",
    "print(\"2. 📉 空间尺寸逐渐减小: 64×64 → 64×64 → 32×32 → 16×16 → 8×8 → 4×4\")\n",
    "print(\"3. 🧠 信息密度逐渐增大: 从像素级特征到抽象语义特征\")\n",
    "print(\"4. 🎯 最终得到64维的紧凑表示，包含了图像的核心信息\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 预处理层的作用验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔬 深入分析预处理层的作用\n",
      "\n",
      "📊 统计信息对比:\n",
      "原始图像统计:\n",
      "  - 通道数: 3\n",
      "  - 尺寸: 64×64\n",
      "  - 数值范围: [-3.906, 4.300]\n",
      "  - 平均值: -0.011\n",
      "\n",
      "预处理后特征图统计:\n",
      "  - 通道数: 32\n",
      "  - 尺寸: 64×64\n",
      "  - 数值范围: [0.000, 2.546]\n",
      "  - 平均值: 0.229\n",
      "\n",
      "🔥 激活分析:\n",
      "  - 活跃通道数: 32/32\n",
      "  - 激活率: 100.0%\n",
      "\n",
      "✨ 预处理层的三大作用:\n",
      "  1. 🎨 特征扩展: 从3个颜色通道扩展到32个特征通道\n",
      "  2. 🔧 维度适配: 为后续残差块提供合适的输入维度\n",
      "  3. 🧹 特征清理: 通过BatchNorm和ReLU标准化和去噪\n"
     ]
    }
   ],
   "source": [
    "print(\"🔬 深入分析预处理层的作用\\n\")\n",
    "\n",
    "# 分析预处理层的具体作用\n",
    "input_image = sample_image[0]  # 取第一张图像 (3, 64, 64)\n",
    "preconv_output = layer_outputs['preconv'][0]  # (32, 64, 64)\n",
    "\n",
    "print(\"📊 统计信息对比:\")\n",
    "print(f\"原始图像统计:\")\n",
    "print(f\"  - 通道数: {input_image.shape[0]}\")\n",
    "print(f\"  - 尺寸: {input_image.shape[1]}×{input_image.shape[2]}\")\n",
    "print(f\"  - 数值范围: [{input_image.min():.3f}, {input_image.max():.3f}]\")\n",
    "print(f\"  - 平均值: {input_image.mean():.3f}\")\n",
    "\n",
    "print(f\"\\n预处理后特征图统计:\")\n",
    "print(f\"  - 通道数: {preconv_output.shape[0]}\")\n",
    "print(f\"  - 尺寸: {preconv_output.shape[1]}×{preconv_output.shape[2]}\")\n",
    "print(f\"  - 数值范围: [{preconv_output.min():.3f}, {preconv_output.max():.3f}]\")\n",
    "print(f\"  - 平均值: {preconv_output.mean():.3f}\")\n",
    "\n",
    "# 分析特征图的激活情况\n",
    "active_channels = (preconv_output.mean(dim=[1,2]) > 0.1).sum().item()\n",
    "print(f\"\\n🔥 激活分析:\")\n",
    "print(f\"  - 活跃通道数: {active_channels}/{preconv_output.shape[0]}\")\n",
    "print(f\"  - 激活率: {active_channels/preconv_output.shape[0]*100:.1f}%\")\n",
    "\n",
    "print(f\"\\n✨ 预处理层的三大作用:\")\n",
    "print(f\"  1. 🎨 特征扩展: 从3个颜色通道扩展到32个特征通道\")\n",
    "print(f\"  2. 🔧 维度适配: 为后续残差块提供合适的输入维度\")\n",
    "print(f\"  3. 🧹 特征清理: 通过BatchNorm和ReLU标准化和去噪\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 对比实验：有无预处理层的差异"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚖️  对比实验：有无预处理层的差异\n",
      "\n",
      "📊 模型对比:\n",
      "  有预处理层:   860,896 参数\n",
      "  无预处理层:   767,296 参数\n",
      "  参数差异:     93,600 参数\n",
      "\n",
      "🎯 输出对比:\n",
      "  有预处理层输出: torch.Size([1, 64])\n",
      "  无预处理层输出: torch.Size([1, 64])\n",
      "\n",
      "🤔 思考：\n",
      "  - 预处理层增加了少量参数，但提供了更平滑的特征学习路径\n",
      "  - 直接从3通道跳到64通道，可能导致训练不稳定\n",
      "  - 预处理层就像一个'适配器'，让网络更容易学习\n"
     ]
    }
   ],
   "source": [
    "class ResNetWithoutPreconv(nn.Module):\n",
    "    \"\"\"没有预处理层的ResNet，用于对比\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 直接从3通道开始\n",
    "        self.layer0 = Residual_block(3, 64, stride=2)    # 3→64 大跨步！\n",
    "        self.layer1 = Residual_block(64, 128, stride=2)  # 64→128\n",
    "        self.layer2 = Residual_block(128, 128, stride=2) # 128→128\n",
    "        self.layer3 = Residual_block(128, 64, stride=2)  # 128→64\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64*4*4, 64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer0(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# 创建对比实验\n",
    "resnet_with_preconv = ResNetEncoder()\n",
    "resnet_without_preconv = ResNetWithoutPreconv()\n",
    "\n",
    "test_input = torch.randn(1, 3, 64, 64)\n",
    "\n",
    "print(\"⚖️  对比实验：有无预处理层的差异\\n\")\n",
    "\n",
    "# 计算参数量\n",
    "params_with = sum(p.numel() for p in resnet_with_preconv.parameters())\n",
    "params_without = sum(p.numel() for p in resnet_without_preconv.parameters())\n",
    "\n",
    "print(f\"📊 模型对比:\")\n",
    "print(f\"  有预处理层:   {params_with:,} 参数\")\n",
    "print(f\"  无预处理层:   {params_without:,} 参数\")\n",
    "print(f\"  参数差异:     {params_with - params_without:,} 参数\")\n",
    "\n",
    "# 测试前向传播\n",
    "with torch.no_grad():\n",
    "    output_with, _ = resnet_with_preconv(test_input)\n",
    "    output_without = resnet_without_preconv(test_input)\n",
    "\n",
    "print(f\"\\n🎯 输出对比:\")\n",
    "print(f\"  有预处理层输出: {output_with.shape}\")\n",
    "print(f\"  无预处理层输出: {output_without.shape}\")\n",
    "\n",
    "print(f\"\\n🤔 思考：\")\n",
    "print(f\"  - 预处理层增加了少量参数，但提供了更平滑的特征学习路径\")\n",
    "print(f\"  - 直接从3通道跳到64通道，可能导致训练不稳定\")\n",
    "print(f\"  - 预处理层就像一个'适配器'，让网络更容易学习\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 完整的ResNet自编码器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 完整ResNet自编码器测试结果:\n",
      "  输入图像:     torch.Size([2, 3, 64, 64])\n",
      "  编码特征:     torch.Size([2, 64])\n",
      "  重构图像:     torch.Size([2, 3, 64, 64])\n",
      "  压缩比率:     192.0:1\n",
      "  总参数量:     1,592,163\n",
      "\n",
      "✨ 这就是异常检测的原理:\n",
      "  1. 正常图像 → 编码 → 解码 → 重构误差小\n",
      "  2. 异常图像 → 编码 → 解码 → 重构误差大\n",
      "  3. 通过重构误差判断图像是否异常！\n"
     ]
    }
   ],
   "source": [
    "class CompleteResNetAutoencoder(nn.Module):\n",
    "    \"\"\"完整的ResNet自编码器 - 编码器 + 解码器\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 编码器部分（我们刚才详细分析的）\n",
    "        self.encoder = ResNetEncoder()\n",
    "        \n",
    "        # 解码器部分 - 从64维特征重构回64x64x3图像\n",
    "        self.decoder = nn.Sequential(\n",
    "            # 64维 → 64*4*4维\n",
    "            nn.Linear(64, 64*4*4),\n",
    "            nn.BatchNorm1d(64*4*4),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # 重塑为4D张量\n",
    "            nn.Unflatten(1, (64, 4, 4)),  # (N, 64*4*4) → (N, 64, 4, 4)\n",
    "            \n",
    "            # 转置卷积，逐步放大\n",
    "            nn.ConvTranspose2d(64, 128, 4, stride=2, padding=1),   # 4×4 → 8×8\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.ConvTranspose2d(128, 128, 4, stride=2, padding=1),  # 8×8 → 16×16\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.ConvTranspose2d(128, 128, 4, stride=2, padding=1),  # 16×16 → 32×32\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.ConvTranspose2d(128, 3, 4, stride=2, padding=1),    # 32×32 → 64×64\n",
    "            nn.Tanh()  # 输出范围[-1, 1]\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 编码\n",
    "        encoded, encoder_outputs = self.encoder(x)\n",
    "        \n",
    "        # 解码\n",
    "        decoded = self.decoder(encoded)\n",
    "        \n",
    "        return decoded, encoded, encoder_outputs\n",
    "\n",
    "# 测试完整的自编码器\n",
    "autoencoder = CompleteResNetAutoencoder()\n",
    "test_image = torch.randn(2, 3, 64, 64)  # 2张测试图像\n",
    "\n",
    "reconstructed, encoded, encoder_details = autoencoder(test_image)\n",
    "\n",
    "print(\"🎯 完整ResNet自编码器测试结果:\")\n",
    "print(f\"  输入图像:     {test_image.shape}\")\n",
    "print(f\"  编码特征:     {encoded.shape}\")\n",
    "print(f\"  重构图像:     {reconstructed.shape}\")\n",
    "print(f\"  压缩比率:     {test_image.numel() / encoded.numel():.1f}:1\")\n",
    "\n",
    "total_params = sum(p.numel() for p in autoencoder.parameters())\n",
    "print(f\"  总参数量:     {total_params:,}\")\n",
    "\n",
    "print(f\"\\n✨ 这就是异常检测的原理:\")\n",
    "print(f\"  1. 正常图像 → 编码 → 解码 → 重构误差小\")\n",
    "print(f\"  2. 异常图像 → 编码 → 解码 → 重构误差大\")\n",
    "print(f\"  3. 通过重构误差判断图像是否异常！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 总结 - 预处理层的重要性\n",
    "\n",
    "通过以上实验，我们深入理解了ResNet中预处理卷积层的作用："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📚 总结：为什么需要预处理卷积层？\n",
      "\n",
      "🎯 核心作用:\n",
      "  1. 🌉 平滑过渡: 3通道 → 32通道 → 64通道 → ..., 避免维度跳跃过大\n",
      "  2. 🎨 特征提升: 从RGB像素提升到可学习的特征表示\n",
      "  3. 🔧 维度适配: 为残差块提供合适的输入通道数\n",
      "  4. 🧹 数据预处理: BatchNorm + ReLU 标准化和激活\n",
      "\n",
      "📊 具体效果:\n",
      "  • 输入: (N, 3, 64, 64) - 原始RGB图像\n",
      "  • 输出: (N, 32, 64, 64) - 32个特征图\n",
      "  • 参数: 3×32×3×3 = 864个参数（很少！）\n",
      "  • 好处: 为整个网络奠定良好的基础\n",
      "\n",
      "🔥 类比理解:\n",
      "  预处理层就像学习过程中的'预备课'：\n",
      "  • 没有预处理: 直接从小学跳到大学 😵\n",
      "  • 有预处理: 小学 → 初中 → 高中 → 大学 ✅\n",
      "  • 结果: 循序渐进，学习效果更好！\n",
      "\n",
      "🎓 学习要点:\n",
      "  1. 深度学习中，渐进式的特征提取比跳跃式更有效\n",
      "  2. 少量参数的预处理层能带来显著的训练稳定性\n",
      "  3. ResNet的成功离不开这种精心设计的架构\n",
      "  4. 在设计网络时，要考虑特征流动的平滑性\n"
     ]
    }
   ],
   "source": [
    "print(\"📚 总结：为什么需要预处理卷积层？\\n\")\n",
    "\n",
    "print(\"🎯 核心作用:\")\n",
    "print(\"  1. 🌉 平滑过渡: 3通道 → 32通道 → 64通道 → ..., 避免维度跳跃过大\")\n",
    "print(\"  2. 🎨 特征提升: 从RGB像素提升到可学习的特征表示\")\n",
    "print(\"  3. 🔧 维度适配: 为残差块提供合适的输入通道数\")\n",
    "print(\"  4. 🧹 数据预处理: BatchNorm + ReLU 标准化和激活\")\n",
    "\n",
    "print(\"\\n📊 具体效果:\")\n",
    "print(\"  • 输入: (N, 3, 64, 64) - 原始RGB图像\")\n",
    "print(\"  • 输出: (N, 32, 64, 64) - 32个特征图\")\n",
    "print(\"  • 参数: 3×32×3×3 = 864个参数（很少！）\")\n",
    "print(\"  • 好处: 为整个网络奠定良好的基础\")\n",
    "\n",
    "print(\"\\n🔥 类比理解:\")\n",
    "print(\"  预处理层就像学习过程中的'预备课'：\")\n",
    "print(\"  • 没有预处理: 直接从小学跳到大学 😵\")\n",
    "print(\"  • 有预处理: 小学 → 初中 → 高中 → 大学 ✅\")\n",
    "print(\"  • 结果: 循序渐进，学习效果更好！\")\n",
    "\n",
    "print(\"\\n🎓 学习要点:\")\n",
    "print(\"  1. 深度学习中，渐进式的特征提取比跳跃式更有效\")\n",
    "print(\"  2. 少量参数的预处理层能带来显著的训练稳定性\")\n",
    "print(\"  3. ResNet的成功离不开这种精心设计的架构\")\n",
    "print(\"  4. 在设计网络时，要考虑特征流动的平滑性\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (Lee Course)",
   "language": "python",
   "name": "lee_course_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
