{"cells":[{"cell_type":"markdown","metadata":{"id":"xvSGDbExff_I"},"source":["# **Homework 7 - Bert (Question Answering)**\n","\n","If you have any questions, feel free to email us at mlta-2022-spring@googlegroups.com\n","\n","\n","\n","Slide:    [Link](https://docs.google.com/presentation/d/1H5ZONrb2LMOCixLY7D5_5-7LkIaXO6AGEaV2mRdTOMY/edit?usp=sharing)　Kaggle: [Link](https://www.kaggle.com/c/ml2022spring-hw7)　Data: [Link](https://drive.google.com/uc?id=1AVgZvy3VFeg0fX-6WQJMHPVrx3A-M1kb)\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"WGOr_eS3wJJf"},"source":["## Task description\n","- Chinese Extractive Question Answering\n","  - Input: Paragraph + Question\n","  - Output: Answer\n","\n","- Objective: Learn how to fine tune a pretrained model on downstream task using transformers\n","\n","- Todo\n","    - Fine tune a pretrained chinese BERT model\n","    - Change hyperparameters (e.g. doc_stride)\n","    - Apply linear learning rate decay\n","    - Try other pretrained models\n","    - Improve preprocessing\n","    - Improve postprocessing\n","- Training tips\n","    - Automatic mixed precision\n","    - Gradient accumulation\n","    - Ensemble\n","\n","- Estimated training time (tesla t4 with automatic mixed precision enabled)\n","    - Simple: 8mins\n","    - Medium: 8mins\n","    - Strong: 25mins\n","    - Boss: 2.5hrs\n","  "]},{"cell_type":"markdown","metadata":{"id":"TJ1fSAJE2oaC"},"source":["## Download Dataset"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2931,"status":"ok","timestamp":1756861841962,"user":{"displayName":"湯琴","userId":"00701203190468333804"},"user_tz":-540},"id":"EMkg2396HH53","outputId":"10a5a8f9-7184-4130-cf12-e8723552c229"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":71,"status":"ok","timestamp":1756861842029,"user":{"displayName":"湯琴","userId":"00701203190468333804"},"user_tz":-540},"id":"YPrc4Eie9Yo5","outputId":"422b94e3-7b3b-4e44-e6ba-f8b95bbbd8d8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Wed Sep  3 01:10:42 2025       \n","+-----------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n","|-----------------------------------------+------------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                        |               MIG M. |\n","|=========================================+========================+======================|\n","|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n","| N/A   35C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n","|                                         |                        |                  N/A |\n","+-----------------------------------------+------------------------+----------------------+\n","                                                                                         \n","+-----------------------------------------------------------------------------------------+\n","| Processes:                                                                              |\n","|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n","|        ID   ID                                                               Usage      |\n","|=========================================================================================|\n","|  No running processes found                                                             |\n","+-----------------------------------------------------------------------------------------+\n"]}],"source":["# Download link 1\n","#!gdown --id '1AVgZvy3VFeg0fX-6WQJMHPVrx3A-M1kb' --output hw7_data.zip\n","# Download Link 2 (if the above link fails)\n","# !gdown --id '1qwjbRjq481lHsnTrrF4OjKQnxzgoLEFR' --output hw7_data.zip\n","# Download Link 3 (if the above link fails)\n","# !gdown --id '1QXuWjNRZH6DscSd6QcRER0cnxmpZvijn' --output hw7_data.zip\n","#!unzip -o hw7_data.zip\n","# For this HW, K80 \u003c P4 \u003c T4 \u003c P100 \u003c= T4(fp16) \u003c V100\n","!nvidia-smi"]},{"cell_type":"markdown","metadata":{"id":"TevOvhC03m0h"},"source":["## Install transformers\n","\n","Documentation for the toolkit:　https://huggingface.co/transformers/"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6031,"status":"ok","timestamp":1756861848062,"user":{"displayName":"湯琴","userId":"00701203190468333804"},"user_tz":-540},"id":"IVHw2UyGHH54","outputId":"afa1e43b-c5b2-4d00-8876-bb6928fa6651"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.55.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.19.1)\n","Requirement already satisfied: huggingface-hub\u003c1.0,\u003e=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.34.4)\n","Requirement already satisfied: numpy\u003e=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n","Requirement already satisfied: packaging\u003e=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n","Requirement already satisfied: pyyaml\u003e=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n","Requirement already satisfied: tokenizers\u003c0.22,\u003e=0.21 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.21.4)\n","Requirement already satisfied: safetensors\u003e=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n","Requirement already satisfied: tqdm\u003e=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n","Requirement already satisfied: fsspec\u003e=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub\u003c1.0,\u003e=0.34.0-\u003etransformers) (2025.3.0)\n","Requirement already satisfied: typing-extensions\u003e=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub\u003c1.0,\u003e=0.34.0-\u003etransformers) (4.15.0)\n","Requirement already satisfied: hf-xet\u003c2.0.0,\u003e=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub\u003c1.0,\u003e=0.34.0-\u003etransformers) (1.1.8)\n","Requirement already satisfied: charset_normalizer\u003c4,\u003e=2 in /usr/local/lib/python3.12/dist-packages (from requests-\u003etransformers) (3.4.3)\n","Requirement already satisfied: idna\u003c4,\u003e=2.5 in /usr/local/lib/python3.12/dist-packages (from requests-\u003etransformers) (3.10)\n","Requirement already satisfied: urllib3\u003c3,\u003e=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests-\u003etransformers) (2.5.0)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests-\u003etransformers) (2025.8.3)\n"]}],"source":["!pip install transformers"]},{"cell_type":"markdown","metadata":{"id":"8dKM4yCh4LI_"},"source":["## Import Packages"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":8356,"status":"ok","timestamp":1756861856414,"user":{"displayName":"湯琴","userId":"00701203190468333804"},"user_tz":-540},"id":"WOTHHtWJoahe"},"outputs":[],"source":["import json\n","import numpy as np\n","import random\n","import torch\n","from torch.utils.data import DataLoader, Dataset\n","# from transformers import AdamW, BertForQuestionAnswering, BertTokenizerFast\n","from transformers import BertForQuestionAnswering, BertTokenizerFast\n","from torch.optim import AdamW\n","\n","from tqdm.auto import tqdm\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","# Fix random seed for reproducibility\n","def same_seeds(seed):\n","\t  torch.manual_seed(seed)\n","\t  if torch.cuda.is_available():\n","\t\t    torch.cuda.manual_seed(seed)\n","\t\t    torch.cuda.manual_seed_all(seed)\n","\t  np.random.seed(seed)\n","\t  random.seed(seed)\n","\t  torch.backends.cudnn.benchmark = False\n","\t  torch.backends.cudnn.deterministic = True\n","same_seeds(0)"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1756861856420,"user":{"displayName":"湯琴","userId":"00701203190468333804"},"user_tz":-540},"id":"7pBtSZP1SKQO"},"outputs":[],"source":["# Change \"fp16_training\" to True to support automatic mixed precision training (fp16)\n","fp16_training = False\n","\n","if fp16_training:\n","    !pip install accelerate==0.2.0\n","    from accelerate import Accelerator\n","    accelerator = Accelerator(fp16==True)\n","    device = accelerator.device\n","\n","# Documentation for the toolkit:  https://huggingface.co/docs/accelerate/"]},{"cell_type":"markdown","metadata":{"id":"2YgXHuVLp_6j"},"source":["## Load Model and Tokenizer\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":234},"executionInfo":{"elapsed":4586,"status":"ok","timestamp":1756861861010,"user":{"displayName":"湯琴","userId":"00701203190468333804"},"user_tz":-540},"id":"xyBCYGjAp3ym","outputId":"31f44823-8184-4de1-cafb-f6de144bc0fa"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n","Some weights of the model checkpoint at luhua/chinese_pretrain_mrc_macbert_large were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n","- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4c2882ea8aa24d0a813a94a2bdb14592","version_major":2,"version_minor":0},"text/plain":["model.safetensors:  15%|#5        | 199M/1.30G [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# model = BertForQuestionAnswering.from_pretrained(\"bert-base-chinese\").to(device)\n","# tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-chinese\")\n","model_name = \"luhua/chinese_pretrain_mrc_macbert_large\"\n","model = BertForQuestionAnswering.from_pretrained(model_name).to(device)\n","tokenizer = BertTokenizerFast.from_pretrained(model_name)"]},{"cell_type":"markdown","metadata":{"id":"3Td-GTmk5OW4"},"source":["## Read Data\n","\n","- Training set: 31690 QA pairs\n","- Dev set: 4131  QA pairs\n","- Test set: 4957  QA pairs\n","\n","- {train/dev/test}_questions:\n","  - List of dicts with the following keys:\n","   - id (int)\n","   - paragraph_id (int)\n","   - question_text (string)\n","   - answer_text (string)\n","   - answer_start (int)\n","   - answer_end (int)\n","- {train/dev/test}_paragraphs:\n","  - List of strings\n","  - paragraph_ids in questions correspond to indexs in paragraphs\n","  - A paragraph may be used by several questions"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":3180,"status":"ok","timestamp":1756861864193,"user":{"displayName":"湯琴","userId":"00701203190468333804"},"user_tz":-540},"id":"NvX7hlepogvu"},"outputs":[],"source":["def read_data(file):\n","    with open(file, 'r', encoding=\"utf-8\") as reader:\n","        data = json.load(reader)\n","    return data[\"questions\"], data[\"paragraphs\"]\n","\n","train_questions, train_paragraphs = read_data(\"/content/drive/MyDrive/hw7_train.json\")\n","dev_questions, dev_paragraphs = read_data(\"/content/drive/MyDrive/hw7_dev.json\")\n","test_questions, test_paragraphs = read_data(\"/content/drive/MyDrive/hw7_test.json\")"]},{"cell_type":"markdown","metadata":{"id":"Fm0rpTHq0e4N"},"source":["## Tokenize Data"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":12818,"status":"ok","timestamp":1756861877026,"user":{"displayName":"湯琴","userId":"00701203190468333804"},"user_tz":-540},"id":"rTZ6B70Hoxie"},"outputs":[],"source":["# Tokenize questions and paragraphs separately\n","# 「add_special_tokens」 is set to False since special tokens will be added when tokenized questions and paragraphs are combined in datset __getitem__\n","\n","train_questions_tokenized = tokenizer([train_question[\"question_text\"] for train_question in train_questions], add_special_tokens=False)\n","dev_questions_tokenized = tokenizer([dev_question[\"question_text\"] for dev_question in dev_questions], add_special_tokens=False)\n","test_questions_tokenized = tokenizer([test_question[\"question_text\"] for test_question in test_questions], add_special_tokens=False)\n","\n","train_paragraphs_tokenized = tokenizer(train_paragraphs, add_special_tokens=False)\n","dev_paragraphs_tokenized = tokenizer(dev_paragraphs, add_special_tokens=False)\n","test_paragraphs_tokenized = tokenizer(test_paragraphs, add_special_tokens=False)\n","\n","# You can safely ignore the warning message as tokenized sequences will be futher processed in datset __getitem__ before passing to model"]},{"cell_type":"markdown","metadata":{"id":"Ws8c8_4d5UCI"},"source":["## Dataset and Dataloader"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1756861877043,"user":{"displayName":"湯琴","userId":"00701203190468333804"},"user_tz":-540},"id":"Xjooag-Swnuh"},"outputs":[],"source":["train_batch_size = 8\n","doc_stride = 100\n","\n","class QA_Dataset(Dataset):\n","    def __init__(self, split, questions, tokenized_questions, tokenized_paragraphs, doc_stride=doc_stride):\n","        self.split = split\n","        self.questions = questions\n","        self.tokenized_questions = tokenized_questions\n","        self.tokenized_paragraphs = tokenized_paragraphs\n","        self.max_question_len = 40\n","        self.max_paragraph_len = 350\n","\n","        ##### TODO: Change value of doc_stride #####\n","        self.doc_stride = doc_stride\n","\n","        # Input sequence length = [CLS] + question + [SEP] + paragraph + [SEP]\n","        self.max_seq_len = 1 + self.max_question_len + 1 + self.max_paragraph_len + 1\n","\n","    def __len__(self):\n","        return len(self.questions)\n","\n","    def __getitem__(self, idx):\n","        question = self.questions[idx]\n","        tokenized_question = self.tokenized_questions[idx]\n","        tokenized_paragraph = self.tokenized_paragraphs[question[\"paragraph_id\"]]\n","\n","        ##### TODO: Preprocessing #####\n","        # Hint: How to prevent model from learning something it should not learn\n","\n","        if self.split == \"train\":\n","            # Convert answer's start/end positions in paragraph_text to start/end positions in tokenized_paragraph\n","            answer_start_token = tokenized_paragraph.char_to_token(question[\"answer_start\"])\n","            answer_end_token = tokenized_paragraph.char_to_token(question[\"answer_end\"])\n","\n","            # A single window is obtained by slicing the portion of paragraph containing the answer\n","            #mid = (answer_start_token + answer_end_token) // 2\n","            #paragraph_start = max(0, min(mid - self.max_paragraph_len // 2, len(tokenized_paragraph) - self.max_paragraph_len))\n","            #paragraph_end = paragraph_start + self.max_paragraph_len\n","\n","            start_min = max(0, answer_end_token - self.max_paragraph_len + 1)\n","            start_max = min(answer_start_token, len(tokenized_paragraph) - self.max_paragraph_len)\n","            start_max = max(start_min, start_max)\n","            paragraph_start = random.randint(start_min, start_max + 1)\n","            paragraph_end = paragraph_start + self.max_paragraph_len\n","\n","            # Slice question/paragraph and add special tokens (101: CLS, 102: SEP)\n","            input_ids_question = [101] + tokenized_question.ids[:self.max_question_len] + [102]\n","            input_ids_paragraph = tokenized_paragraph.ids[paragraph_start : paragraph_end] + [102]\n","\n","            # Convert answer's start/end positions in tokenized_paragraph to start/end positions in the window\n","            answer_start_token += len(input_ids_question) - paragraph_start\n","            answer_end_token += len(input_ids_question) - paragraph_start\n","\n","            # Pad sequence and obtain inputs to model\n","            input_ids, token_type_ids, attention_mask = self.padding(input_ids_question, input_ids_paragraph)\n","            return torch.tensor(input_ids), torch.tensor(token_type_ids), torch.tensor(attention_mask), answer_start_token, answer_end_token\n","\n","        # Validation/Testing\n","        else:\n","            input_ids_list, token_type_ids_list, attention_mask_list = [], [], []\n","\n","            # Paragraph is split into several windows, each with start positions separated by step \"doc_stride\"\n","            for i in range(0, len(tokenized_paragraph), self.doc_stride):\n","\n","                # Slice question/paragraph and add special tokens (101: CLS, 102: SEP)\n","                input_ids_question = [101] + tokenized_question.ids[:self.max_question_len] + [102]\n","                input_ids_paragraph = tokenized_paragraph.ids[i : i + self.max_paragraph_len] + [102]\n","\n","                # Pad sequence and obtain inputs to model\n","                input_ids, token_type_ids, attention_mask = self.padding(input_ids_question, input_ids_paragraph)\n","\n","                input_ids_list.append(input_ids)\n","                token_type_ids_list.append(token_type_ids)\n","                attention_mask_list.append(attention_mask)\n","\n","            return torch.tensor(input_ids_list), torch.tensor(token_type_ids_list), torch.tensor(attention_mask_list)\n","\n","    def padding(self, input_ids_question, input_ids_paragraph):\n","        # Pad zeros if sequence length is shorter than max_seq_len\n","        padding_len = self.max_seq_len - len(input_ids_question) - len(input_ids_paragraph)\n","        # Indices of input sequence tokens in the vocabulary\n","        input_ids = input_ids_question + input_ids_paragraph + [0] * padding_len\n","        # Segment token indices to indicate first and second portions of the inputs. Indices are selected in [0, 1]\n","        token_type_ids = [0] * len(input_ids_question) + [1] * len(input_ids_paragraph) + [0] * padding_len\n","        # Mask to avoid performing attention on padding token indices. Mask values selected in [0, 1]\n","        attention_mask = [1] * (len(input_ids_question) + len(input_ids_paragraph)) + [0] * padding_len\n","\n","        return input_ids, token_type_ids, attention_mask\n","\n","\n","\n","train_set = QA_Dataset(\"train\", train_questions, train_questions_tokenized, train_paragraphs_tokenized)\n","dev_set = QA_Dataset(\"dev\", dev_questions, dev_questions_tokenized, dev_paragraphs_tokenized)\n","test_set = QA_Dataset(\"test\", test_questions, test_questions_tokenized, test_paragraphs_tokenized)\n","\n","\n","# Note: Do NOT change batch size of dev_loader / test_loader !\n","# Although batch size=1, it is actually a batch consisting of several windows from the same QA pair\n","train_loader = DataLoader(train_set, batch_size=train_batch_size, shuffle=True, pin_memory=True)\n","dev_loader = DataLoader(dev_set, batch_size=1, shuffle=False, pin_memory=True)\n","test_loader = DataLoader(test_set, batch_size=1, shuffle=False, pin_memory=True)"]},{"cell_type":"markdown","metadata":{"id":"5_H1kqhR8CdM"},"source":["## Function for Evaluation"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":16,"status":"ok","timestamp":1756861877062,"user":{"displayName":"湯琴","userId":"00701203190468333804"},"user_tz":-540},"id":"SqeA3PLPxOHu"},"outputs":[],"source":["def evaluate(data, output, doc_stride=doc_stride, token_type_ids=None, paragraph=None, paragraph_tokenized=None):\n","    ##### TODO: Postprocessing #####\n","    # There is a bug and room for improvement in postprocessing\n","    # Hint: Open your prediction file to see what is wrong\n","\n","    answer = ''\n","    max_prob = float('-inf')\n","    num_of_windows = data[0].shape[1]\n","\n","    for k in range(num_of_windows):\n","        # Obtain answer by choosing the most probable start position / end position\n","        start_prob, start_index = torch.max(output.start_logits[k], dim=0)\n","        end_prob, end_index = torch.max(output.end_logits[k], dim=0)\n","\n","        token_type_id = data[1][0][k].detach().cpu().numpy()\n","        #[CLS] + [question] + [SEP] + [paragraph] + [SEP]\n","        paragraph_start = token_type_id.argmax()\n","        paragraph_end = len(token_type_id) - 1 - token_type_id[::-1].argmax() - 1\n","\n","        if start_index \u003e end_index or start_index \u003c paragraph_start or end_index \u003e paragraph_end:\n","            continue\n","\n","        # Probability of answer is calculated as sum of start_prob and end_prob\n","        prob = start_prob + end_prob\n","\n","        # Replace answer if calculated probability is larger than previous windows\n","        if prob \u003e max_prob:\n","            # Convert tokens to chars (e.g. [1920, 7032] --\u003e \"大 金\")\n","            max_prob = prob\n","            answer = tokenizer.decode(data[0][0][k][start_index : end_index + 1])\n","            # 找到tokenized paragraph中对应的位置\n","            origin_start = start_index + k * doc_stride - paragraph_start\n","            origin_end = end_index + k * doc_stride - paragraph_start;\n","\n","    # Remove spaces in answer (e.g. \"大 金\" --\u003e \"大金\")\n","    answer = answer.replace(' ', '')\n","    if '[UNK]' in answer:\n","        print('发现 [UNK]，这表明有文字无法编码, 使用原始文本')\n","        #print(\"Paragraph:\", paragraph)\n","        #print(\"Paragraph:\", paragraph_tokenized.tokens)\n","        print('--直接解码预测:', answer)\n","        #找到原始文本中对应的位置\n","        raw_start =  paragraph_tokenized.token_to_chars(origin_start)[0]\n","        raw_end = paragraph_tokenized.token_to_chars(origin_end)[1]\n","        answer = paragraph[raw_start:raw_end]\n","        print('--原始文本预测:',answer)\n","\n","    return answer"]},{"cell_type":"markdown","metadata":{"id":"rzHQit6eMnKG"},"source":["## Training"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":875},"id":"3Q-B6ka7xoCM"},"outputs":[{"name":"stdout","output_type":"stream","text":["Start Training ...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e1e6f58dc8234948add320d175a1c189","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/4478 [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 1 | Step 100 | loss = 1.185, acc = 0.606, lr=9.9e-06\n","Epoch 1 | Step 200 | loss = 0.618, acc = 0.725, lr=9.743656136716728e-06\n","Epoch 1 | Step 300 | loss = 0.640, acc = 0.711, lr=9.484722941481098e-06\n","Epoch 1 | Step 400 | loss = 0.660, acc = 0.729, lr=9.22578974624547e-06\n","Epoch 1 | Step 500 | loss = 0.558, acc = 0.761, lr=8.96685655100984e-06\n","Epoch 1 | Step 600 | loss = 0.577, acc = 0.760, lr=8.70792335577421e-06\n","Epoch 1 | Step 700 | loss = 0.602, acc = 0.756, lr=8.448990160538582e-06\n","Epoch 1 | Step 800 | loss = 0.541, acc = 0.779, lr=8.190056965302953e-06\n","Epoch 1 | Step 900 | loss = 0.637, acc = 0.746, lr=7.931123770067325e-06\n","Epoch 1 | Step 1000 | loss = 0.517, acc = 0.786, lr=7.672190574831693e-06\n","Epoch 1 | Step 1100 | loss = 0.553, acc = 0.780, lr=7.413257379596065e-06\n","Epoch 1 | Step 1200 | loss = 0.427, acc = 0.832, lr=7.154324184360436e-06\n","Epoch 1 | Step 1300 | loss = 0.507, acc = 0.784, lr=6.895390989124806e-06\n","Epoch 1 | Step 1400 | loss = 0.524, acc = 0.784, lr=6.6364577938891775e-06\n","Epoch 1 | Step 1500 | loss = 0.443, acc = 0.831, lr=6.377524598653549e-06\n","Epoch 1 | Step 1600 | loss = 0.431, acc = 0.810, lr=6.118591403417918e-06\n","Epoch 1 | Step 1700 | loss = 0.530, acc = 0.785, lr=5.859658208182289e-06\n","Epoch 1 | Step 1800 | loss = 0.470, acc = 0.792, lr=5.60072501294666e-06\n","Epoch 1 | Step 1900 | loss = 0.453, acc = 0.800, lr=5.3417918177110305e-06\n","Epoch 1 | Step 2000 | loss = 0.486, acc = 0.812, lr=5.082858622475402e-06\n","Epoch 1 | Step 2100 | loss = 0.486, acc = 0.801, lr=4.823925427239773e-06\n","Epoch 1 | Step 2200 | loss = 0.446, acc = 0.824, lr=4.564992232004144e-06\n","Epoch 1 | Step 2300 | loss = 0.442, acc = 0.821, lr=4.306059036768514e-06\n","Epoch 1 | Step 2400 | loss = 0.487, acc = 0.791, lr=4.047125841532884e-06\n","Epoch 1 | Step 2500 | loss = 0.396, acc = 0.824, lr=3.788192646297256e-06\n","Epoch 1 | Step 2600 | loss = 0.461, acc = 0.820, lr=3.529259451061626e-06\n","Epoch 1 | Step 2700 | loss = 0.508, acc = 0.799, lr=3.270326255825997e-06\n","Epoch 1 | Step 2800 | loss = 0.430, acc = 0.808, lr=3.011393060590368e-06\n","Epoch 1 | Step 2900 | loss = 0.375, acc = 0.832, lr=2.7524598653547386e-06\n","Epoch 1 | Step 3000 | loss = 0.478, acc = 0.803, lr=2.4935266701191092e-06\n","Epoch 1 | Step 3100 | loss = 0.385, acc = 0.837, lr=2.2345934748834804e-06\n","Epoch 1 | Step 3200 | loss = 0.417, acc = 0.821, lr=1.975660279647851e-06\n","Epoch 1 | Step 3300 | loss = 0.453, acc = 0.797, lr=1.716727084412222e-06\n","Epoch 1 | Step 3400 | loss = 0.428, acc = 0.815, lr=1.4577938891765924e-06\n","Epoch 1 | Step 3500 | loss = 0.415, acc = 0.815, lr=1.1988606939409633e-06\n","Epoch 1 | Step 3600 | loss = 0.439, acc = 0.825, lr=9.399274987053341e-07\n","Epoch 1 | Step 3700 | loss = 0.431, acc = 0.810, lr=6.809943034697049e-07\n","Epoch 1 | Step 3800 | loss = 0.425, acc = 0.825, lr=4.2206110823407565e-07\n","Epoch 1 | Step 3900 | loss = 0.400, acc = 0.839, lr=1.631279129984464e-07\n","Epoch 1 | Step 4000 | loss = 0.400, acc = 0.827, lr=0.0\n","Epoch 1 | Step 4100 | loss = 0.408, acc = 0.804, lr=0.0\n","Epoch 1 | Step 4200 | loss = 0.403, acc = 0.817, lr=0.0\n","Epoch 1 | Step 4300 | loss = 0.429, acc = 0.812, lr=0.0\n","Epoch 1 | Step 4400 | loss = 0.407, acc = 0.825, lr=0.0\n","Saving Model ...\n"]}],"source":["num_epoch = 1\n","validation = False\n","logging_step = 100\n","total_steps = num_epoch * len(train_loader)\n","acc_steps = 1\n","learning_rate = 1e-5\n","optimizer = AdamW(model.parameters(), lr=learning_rate)\n","from transformers import get_linear_schedule_with_warmup\n","scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=100, num_training_steps=total_steps//acc_steps)\n","\n","if fp16_training:\n","    model, optimizer, train_loader = accelerator.prepare(model, optimizer, train_loader)\n","\n","model.train()\n","\n","print(\"Start Training ...\")\n","\n","if not validation:\n","    dev_set = QA_Dataset(\"train\", dev_questions, dev_questions_tokenized, dev_paragraphs_tokenized)\n","    train_set = torch.utils.data.ConcatDataset([train_set, dev_set])\n","    train_loader = DataLoader(train_set, batch_size=train_batch_size, shuffle=True, pin_memory=True)\n","\n","for epoch in range(num_epoch):\n","    step = 1\n","    train_loss = train_acc = 0\n","    optimizer.zero_grad()\n","    for data in tqdm(train_loader):\n","        # Load all data into GPU\n","        data = [i.to(device) for i in data]\n","\n","        # Model inputs: input_ids, token_type_ids, attention_mask, start_positions, end_positions (Note: only \"input_ids\" is mandatory)\n","        # Model outputs: start_logits, end_logits, loss (return when start_positions/end_positions are provided)\n","        output = model(input_ids=data[0], token_type_ids=data[1], attention_mask=data[2], start_positions=data[3], end_positions=data[4])\n","\n","        # Choose the most probable start position / end position\n","        start_index = torch.argmax(output.start_logits, dim=1)\n","        end_index = torch.argmax(output.end_logits, dim=1)\n","\n","        # Prediction is correct only if both start_index and end_index are correct\n","        train_acc += ((start_index == data[3]) \u0026 (end_index == data[4])).float().mean()\n","        train_loss += output.loss\n","\n","        if fp16_training:\n","            accelerator.backward(output.loss)\n","        else:\n","            output.loss.backward()\n","\n","\n","        step += 1\n","        if step % acc_steps == 0:\n","            #grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.1)\n","            optimizer.step()\n","            optimizer.zero_grad()\n","            scheduler.step()\n","\n","        ##### TODO: Apply linear learning rate decay #####\n","\n","        # Print training loss and accuracy over past logging step\n","        if step % logging_step == 0:\n","            lr = optimizer.state_dict()['param_groups'][0]['lr']\n","            print(f\"Epoch {epoch + 1} | Step {step} | loss = {train_loss.item() / logging_step:.3f}, acc = {train_acc / logging_step:.3f}, lr={lr}\")\n","            train_loss = train_acc = 0\n","\n","\n","    if validation:\n","        print(\"Evaluating Dev Set ...\")\n","        model.eval()\n","        with torch.no_grad():\n","            dev_acc = 0\n","            for i, data in enumerate(tqdm(dev_loader)):\n","                output = model(input_ids=data[0].squeeze(dim=0).to(device), token_type_ids=data[1].squeeze(dim=0).to(device),\n","                       attention_mask=data[2].squeeze(dim=0).to(device))\n","                # prediction is correct only if answer text exactly matches\n","                answer = evaluate(data, output, doc_stride=doc_stride,paragraph=dev_paragraphs[dev_questions[i][\"paragraph_id\"]],\n","                                 paragraph_tokenized=dev_paragraphs_tokenized[dev_questions[i][\"paragraph_id\"]])\n","\n","                dev_acc +=  answer == dev_questions[i][\"answer_text\"]\n","\n","            print(f\"Validation | Epoch {epoch + 1} | acc = {dev_acc / len(dev_loader):.3f}\")\n","\n","        model.train()\n","\n","# Save a model and its configuration file to the directory 「saved_model」\n","# i.e. there are two files under the direcory 「saved_model」: 「pytorch_model.bin」 and 「config.json」\n","# Saved model can be re-loaded using 「model = BertForQuestionAnswering.from_pretrained(\"saved_model\")」\n","print(\"Saving Model ...\")\n","model_save_dir = \"/content/drive/MyDrive/saved_model\"\n","model.save_pretrained(model_save_dir)"]},{"cell_type":"markdown","metadata":{"id":"kMmdLOKBMsdE"},"source":["## Testing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U5scNKC9xz0C"},"outputs":[],"source":["print(\"Evaluating Test Set ...\")\n","\n","result = []\n","\n","model.eval()\n","with torch.no_grad():\n","    for i, data in enumerate(tqdm(test_loader)):\n","        output = model(input_ids=data[0].squeeze(dim=0).to(device), token_type_ids=data[1].squeeze(dim=0).to(device),\n","                       attention_mask=data[2].squeeze(dim=0).to(device))\n","        result.append(evaluate(data, output, doc_stride=doc_stride, paragraph=test_paragraphs[test_questions[i][\"paragraph_id\"]],\n","                               paragraph_tokenized=test_paragraphs_tokenized[test_questions[i][\"paragraph_id\"]]))\n","\n","result_file = \"/content/drive/MyDrive/result.csv\"\n","with open(result_file, 'w') as f:\n","\t  f.write(\"ID,Answer\\n\")\n","\t  for i, test_question in enumerate(test_questions):\n","        # Replace commas in answers with empty strings (since csv is separated by comma)\n","\t\t    f.write(f\"{test_question['id']},{result[i].replace(',','')}\\n\")\n","\n","print(f\"Completed! Result is in {result_file}\")"]},{"cell_type":"markdown","metadata":{"id":"a1RItpifHH56"},"source":["参考：https://www.kaggle.com/code/machinehelearning6/notebook01c7eaa0e4"]}],"metadata":{"accelerator":"GPU","colab":{"name":"","version":""},"kernelspec":{"display_name":"Python 3.11 (Lee Course)","language":"python","name":"lee_course_env"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"widgets":{"application/vnd.jupyter.widget-state+json":{"00239c3c65864ff1b0307f8c9ccea3c6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1af81f342b944bb280f35fcc66182d97","placeholder":"​","style":"IPY_MODEL_c41487c38ad24c2b874cb9730b13a044","value":"model.safetensors: 100%"}},"0ec1b0c33189499baee8a2f1b08dab65":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_8c6bddb84ce544d18c2c25d1889a8f2d","max":4478,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b25eae03e4054a75a766b5332f3becb0","value":4405}},"1233e9d3399240d9be5883a6abff44f5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"19da1f6b1f7745e6b5c6a3ba461ba7b5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1af81f342b944bb280f35fcc66182d97":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3cc7cfa8c7874dbcad5d9d2f73d33999":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3d366fe537cf4dd58c1e604e1ef69e4f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a18e875ed0294d0f831fb83c325d1284","placeholder":"​","style":"IPY_MODEL_19da1f6b1f7745e6b5c6a3ba461ba7b5","value":" 98%"}},"3fb36c4c25b446a7ab81fdf05d484c52":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cf1b7ba39ca141e2a160817678f796a4","placeholder":"​","style":"IPY_MODEL_8d0bbe6e0b684ed09c21a52b13853d45","value":" 1.30G/1.30G [00:46\u0026lt;00:00, 24.1MB/s]"}},"4c2882ea8aa24d0a813a94a2bdb14592":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_00239c3c65864ff1b0307f8c9ccea3c6","IPY_MODEL_86b5f64b3e104ceab22ab18c558636e3","IPY_MODEL_3fb36c4c25b446a7ab81fdf05d484c52"],"layout":"IPY_MODEL_e815ff3c907b49b8968b5db72ad603be"}},"86b5f64b3e104ceab22ab18c558636e3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3cc7cfa8c7874dbcad5d9d2f73d33999","max":1302144896,"min":0,"orientation":"horizontal","style":"IPY_MODEL_bc32c33bb5f8450089e42b160d5c333e","value":1302144896}},"8c6bddb84ce544d18c2c25d1889a8f2d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8d0bbe6e0b684ed09c21a52b13853d45":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a18e875ed0294d0f831fb83c325d1284":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a5de86bdf5134121a377af570421e75a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b25eae03e4054a75a766b5332f3becb0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"bc32c33bb5f8450089e42b160d5c333e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c41487c38ad24c2b874cb9730b13a044":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cf1b7ba39ca141e2a160817678f796a4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e1e6f58dc8234948add320d175a1c189":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3d366fe537cf4dd58c1e604e1ef69e4f","IPY_MODEL_0ec1b0c33189499baee8a2f1b08dab65","IPY_MODEL_fb1c2500451a471a8dead4150dbd4578"],"layout":"IPY_MODEL_a5de86bdf5134121a377af570421e75a"}},"e4ad9aeba9144cbe838159f8dd80541a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e815ff3c907b49b8968b5db72ad603be":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fb1c2500451a471a8dead4150dbd4578":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1233e9d3399240d9be5883a6abff44f5","placeholder":"​","style":"IPY_MODEL_e4ad9aeba9144cbe838159f8dd80541a","value":" 4405/4478 [2:18:01\u0026lt;02:16,  1.87s/it]"}}}}},"nbformat":4,"nbformat_minor":0}