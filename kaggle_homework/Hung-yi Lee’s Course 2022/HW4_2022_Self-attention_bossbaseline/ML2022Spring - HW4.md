
# HW 4 Self-attention

HW4 Speaker Identification (Self-attention)

**kaggle url:** [ML2022Spring-hw4](https://www.kaggle.com/competitions/ml2022spring-hw4/overview)
**PDF url:** [HW04.pdf](https://speech.ee.ntu.edu.tw/~hylee/ml/ml2022-course-data/Machine%20Learning%20HW4.pdf)

### Objectives

ãƒ»Task: Multiclass Classification
ãƒ»Predict speaker class from given speech
ãƒ»Goal: Learn how to use Transformer.

### Grading -- Kaggle and Hints
```
---- simple baseline ----
Score: 0.60824

---- medium baseline ----
Score: 0.70375
Modify the parameters of the transformer modules in the sample code

---- strong baseline ----
Score:0.77750
Construct Conformer, which is a variety of Transformer.

---- boss baseline ----
Score:0.86500
Implement Self-Attention Pooling & Additive Margin Softmax to further
boost the performance.
```
>è¿™é‡Œæ²¡æœ‰æŒ‰ç…§è¦æ±‚æ¥åšï¼Œç›´æ¥ç”¨çš„é¢„è®­ç»ƒæ¨¡å‹

### æ•°æ®ç»“æ„
æ•°æ®æµè½¬è¿‡ç¨‹ï¼š
```python
# 1. åŸå§‹éŸ³é¢‘æ–‡ä»¶ï¼ˆæ¯”å¦‚ speaker_A_001.wavï¼‰
# é€šè¿‡ log_melspectrogram.pt ,wav2mel.pt å’Œ sox_effects.pt å¤„ç†
# æ”¶é›†wavéŸ³é¢‘æ•°æ®
#    â†“
# ä½¿ç”¨æä¾›çš„å·¥å…·å¤„ç† (log_melspectrogram.pt + wav2mel.pt + sox_effects.pt)
#    â†“
# ç”Ÿæˆ.ptç‰¹å¾æ–‡ä»¶å’Œjsonå…ƒæ•°æ®

# 2. ä¿å­˜ä¸ºç‰¹å¾æ–‡ä»¶ï¼ˆuttr-xxxxx.ptï¼‰
# melé¢‘è°±å›¾é€šå¸¸ç”¨40ä¸ªé¢‘ç‡å¸¦æ¥è¡¨ç¤ºè¯­éŸ³
mel_features = torch.tensor([
    [-0.065, -0.585, 2.407, ...],  # å¸§1: 40ç»´ç‰¹å¾
    [-1.839, -1.247, 1.529, ...],  # å¸§2: 40ç»´ç‰¹å¾
    # ... æ€»å…±Nå¸§
])  # å½¢çŠ¶: (N, 40)

# 3. åœ¨metadata.jsonä¸­è®°å½•
{
  "speakers": {
    "id03074": [
      {
        "feature_path": "uttr-xxxxx.pt",  # æŒ‡å‘ä¸Šé¢çš„ç‰¹å¾æ–‡ä»¶
        "mel_len": N                      # å¸§æ•°
      }
    ]
  }
}

# 4. è®­ç»ƒæ—¶ï¼ŒmyDatasetç±»è¯»å–æ•°æ®ï¼š
def __getitem__(self, index):
    feat_path, speaker = self.data[index]  # è·å–æ–‡ä»¶è·¯å¾„å’Œè¯´è¯äººID
    mel = torch.load(feat_path)            # åŠ è½½melç‰¹å¾ (N, 40)
    
    # éšæœºæˆªå–128å¸§
    # 128å¸§çº¦ç­‰äº3.2ç§’çš„è¯­éŸ³ï¼ˆå‡è®¾æ¯å¸§25msï¼‰
    if len(mel) > 128:
        start = random.randint(0, len(mel) - 128)
        mel = mel[start:start+128]         # (128, 40)
    
    speaker_id = self.speaker2id[speaker]  # è½¬æ¢ä¸ºæ•°å­—æ ‡ç­¾
    return mel, speaker_id

# 5. æ¨¡å‹å¤„ç†ï¼š
# mel: (batch_size, 128, 40) â†’ prenet â†’ (batch_size, 128, 80)
# â†’ transformer â†’ (batch_size, 128, 80) â†’ mean_pooling â†’ (batch_size, 80)
# â†’ pred_layer â†’ (batch_size, n_speakers)
```

### ğŸ“ **æ•°æ®ç»“æ„è¯¦è§£**

1. **mapping.json** - è¯´è¯äººæ˜ å°„æ–‡ä»¶

```json
{
  "speaker2id": {
    "id00464": 0,    *// è¯´è¯äºº"id00464"å¯¹åº”æ•°å­—æ ‡ç­¾0*
    "id00559": 1,    *// è¯´è¯äºº"id00559"å¯¹åº”æ•°å­—æ ‡ç­¾1*
    "id00578": 2,    *// è¯´è¯äºº"id00578"å¯¹åº”æ•°å­—æ ‡ç­¾2*
    ...
  }
}
```

**ä½œç”¨**ï¼š

- å°†è¯´è¯äººçš„å­—ç¬¦ä¸²IDè½¬æ¢ä¸ºæ•°å­—æ ‡ç­¾
- æ¨¡å‹è®­ç»ƒæ—¶éœ€è¦æ•°å­—æ ‡ç­¾ï¼Œè€Œä¸æ˜¯å­—ç¬¦ä¸²
- æ¨ç†æ—¶éœ€è¦åå‘æ˜ å°„ï¼ˆæ•°å­—â†’å§“åï¼‰

2. **metadata.json** - è®­ç»ƒæ•°æ®å…ƒä¿¡æ¯

```json
{
  "n_mels": 40,           *// melé¢‘è°±å›¾çš„ç‰¹å¾ç»´åº¦ï¼ˆ40ç»´ï¼‰*
  "speakers": {
    "id03074": [          *// è¯´è¯äººID*
      {
        "feature_path": "uttr-18e375195dc146fd8d14b8a322c29b90.pt",  *// ç‰¹å¾æ–‡ä»¶è·¯å¾„*
        "mel_len": 435    *// è¯¥éŸ³é¢‘çš„å¸§æ•°é•¿åº¦*
      },
      {
        "feature_path": "uttr-da9917d5853049178487c065c9e8b718.pt",
        "mel_len": 490
      }
      *// ... è¯¥è¯´è¯äººçš„å…¶ä»–éŸ³é¢‘æ–‡ä»¶*
    ]
  }
}
```

**ä½œç”¨**ï¼š

- è®°å½•æ¯ä¸ªè¯´è¯äººæœ‰å“ªäº›éŸ³é¢‘æ–‡ä»¶
- è®°å½•æ¯ä¸ªéŸ³é¢‘çš„é•¿åº¦ä¿¡æ¯
- è®­ç»ƒæ—¶ç”¨æ¥æ„å»ºæ•°æ®é›†

3. **testdata.json** - æµ‹è¯•æ•°æ®å…ƒä¿¡æ¯

```json
{
  "n_mels": 40,
  "utterances": [
    {
      "feature_path": "uttr-b52ddeaacf1b42ff9c947eadce3e1966.pt",
      "mel_len": 813
    },
    {
      "feature_path": "uttr-fc88b32cb5c94af6817ec97e0a145d74.pt", 
      "mel_len": 738
    }
    *// ... æ›´å¤šæµ‹è¯•éŸ³é¢‘*
  ]
}
```

**ä½œç”¨**ï¼š

- è®°å½•æ‰€æœ‰éœ€è¦é¢„æµ‹çš„éŸ³é¢‘æ–‡ä»¶
- æ³¨æ„ï¼šæ²¡æœ‰è¯´è¯äººæ ‡ç­¾ï¼ˆå› ä¸ºè¿™å°±æ˜¯æˆ‘ä»¬è¦é¢„æµ‹çš„ï¼‰

4. **uttr-xxxxxx.ptæ–‡ä»¶** - å®é™…çš„éŸ³é¢‘ç‰¹å¾

```python
*# è¿™æ˜¯ä¸€ä¸ªå½¢çŠ¶ä¸º (æ—¶é—´å¸§æ•°, 40) çš„äºŒç»´å¼ é‡*
tensor([
  [-6.5321e-02, -5.8502e-01, 2.4070e+00, ..., -2.5698e+00],  *# ç¬¬1å¸§çš„40ç»´ç‰¹å¾*
  [-1.8387e+00, -1.2468e+00, 1.5288e+00, ..., -3.0427e+00],  *# ç¬¬2å¸§çš„40ç»´ç‰¹å¾# ... æ›´å¤šæ—¶é—´å¸§*
])
```

**è¯¦ç»†è§£é‡Š**ï¼š

- **æ—¶é—´ç»´åº¦**ï¼šæ¯ä¸€è¡Œä»£è¡¨ä¸€ä¸ªæ—¶é—´å¸§ï¼ˆé€šå¸¸25msï¼‰
- **ç‰¹å¾ç»´åº¦**ï¼šæ¯ä¸€åˆ—ä»£è¡¨ä¸€ä¸ªmelé¢‘ç‡å¸¦çš„èƒ½é‡å€¼
- **æ•°å€¼å«ä¹‰**ï¼šå¯¹æ•°melé¢‘è°±å›¾çš„èƒ½é‡å€¼
    - æ­£å€¼ï¼šè¯¥é¢‘ç‡å¸¦èƒ½é‡è¾ƒå¼º
    - è´Ÿå€¼ï¼šè¯¥é¢‘ç‡å¸¦èƒ½é‡è¾ƒå¼±
    - æ•°å€¼èŒƒå›´å¤§è‡´åœ¨-4åˆ°+4ä¹‹é—´


### ä¼˜åŒ–æ–¹å‘:


**1.æ•°æ®å’Œè®­ç»ƒå‚æ•°è°ƒæ•´:**
```python
def __init__(self, data_dir, segment_len=192):  # åŸç‰ˆ128 -> 192# åŸå› ï¼šæ›´é•¿åºåˆ—æä¾›æ›´å¤šä¸Šä¸‹æ–‡ï¼Œä½†ä¸ä¼šå¤ªé•¿å¯¼è‡´å†…å­˜é—®é¢˜# åœ¨ parse_args() ä¸­
config = {
    # å­¦ä¹ ç‡ = (å½“å‰æ­¥æ•° / warmup_steps) Ã— ç›®æ ‡å­¦ä¹ ç‡
    "warmup_steps": 2000,    # åŸç‰ˆ1000 -> 2000 (æ›´å¹³æ»‘çš„å­¦ä¹ ç‡é¢„çƒ­)
    "total_steps": 200000,   # åŸç‰ˆ70000 -> 200000 (é€‚å½“å¢åŠ è®­ç»ƒ)
}
```
è®¡ç®—æ€» epochs æ•°ï¼š
```
è¦çŸ¥é“è®­ç»ƒäº†å¤šå°‘ä¸ª epochï¼Œéœ€è¦è¿™ä¸ªå…¬å¼ï¼š
æ€»epochs = total_steps / steps_per_epoch
å…¶ä¸­ steps_per_epoch = è®­ç»ƒé›†å¤§å° / batch_size
ç”±äºä»£ç ä¸­è®­ç»ƒé›†æ˜¯æ€»æ•°æ®çš„90%ï¼š
pythontrainlen = int(0.9 * len(dataset))
```

**2.æ¨¡å‹ç»´åº¦å‡çº§:**
```python
class Classifier(nn.Module):
    def __init__(self, d_model=160, n_spks=600, dropout=0.1):  # åŸç‰ˆ80 -> 200
        # 200ç»´çš„ä¼˜åŠ¿ï¼š
        # èƒ½å­¦åˆ°æ›´ç»†è‡´çš„ç‰¹å¾
        # æ¯”å¦‚ï¼šè¯´è¯çš„èŠ‚å¥ã€å£éŸ³ç»†èŠ‚ã€æƒ…æ„Ÿè‰²å½©ç­‰
        # æ›´å®¹æ˜“åŒºåˆ†ç›¸ä¼¼çš„è¯´è¯äºº
```



**3.æ—©åœæœºåˆ¶**
```python
# åœ¨mainå‡½æ•°ä¸­æ·»åŠ 
def main(..., early_stop=15):  # è¿™é‡Œä½¿ç”¨15
    best_accuracy = -1.0
    best_state_dict = None
    early_stop_count = 0
    
    for step in range(total_steps):
        # è®­ç»ƒä»£ç ...
        
        if (step + 1) % valid_steps == 0:
            valid_accuracy = valid(valid_loader, model, criterion, device)
            
            if valid_accuracy > best_accuracy:
                best_accuracy = valid_accuracy
                best_state_dict = model.state_dict()
                early_stop_count = 0  # é‡ç½®è®¡æ•°
            else:
                early_stop_count += 1
                
            if early_stop_count >= early_stop:
                print(f"Early stopping at step {step+1}")
                break
```

**4.æ·»åŠ SpecAugåˆ°Dataset**
```python
# åœ¨åŸç‰ˆçš„ myDataset ç±»ä¸­æ·»åŠ 
import random

class myDataset(Dataset):
    def __init__(self, data_dir, segment_len=192, use_spec_aug=True):  # æ·»åŠ spec_augå‚æ•°
        self.data_dir = data_dir
        self.segment_len = segment_len
        self.use_spec_aug = use_spec_aug  # æ–°å¢
    
    def spec_augment(self, mel_spectrogram, 
                    time_mask_num=2, time_mask_width=10,
                    freq_mask_num=2, freq_mask_width=8):
        """æ–°å¢SpecAugå‡½æ•°"""
        time_steps, n_mels = mel_spectrogram.shape
        augmented_mel = mel_spectrogram.clone()
        
        # Time Masking
        for _ in range(time_mask_num):
            t = random.randint(0, min(time_mask_width, time_steps))
            t0 = random.randint(0, max(1, time_steps - t))
            augmented_mel[t0:t0+t, :] = -20  # melçš„paddingå€¼
        
        # Frequency Masking  
        for _ in range(freq_mask_num):
            f = random.randint(0, min(freq_mask_width, n_mels))
            f0 = random.randint(0, max(1, n_mels - f))
            augmented_mel[:, f0:f0+f] = -20
            
        return augmented_mel

    def __getitem__(self, index):
        feat_path, speaker = self.data[index]
        mel = torch.load(os.path.join(self.data_dir, feat_path))

        # åŸæ¥çš„åˆ†æ®µé€»è¾‘ä¿æŒä¸å˜
        if len(mel) > self.segment_len:
            start = random.randint(0, len(mel) - self.segment_len)
            mel = torch.FloatTensor(mel[start:start+self.segment_len])
        else:
            mel = torch.FloatTensor(mel)
        
        # æ–°å¢ï¼šSpecAugï¼ˆåªåœ¨è®­ç»ƒæ—¶ä½¿ç”¨ï¼‰
        if self.use_spec_aug and hasattr(self, 'training') and self.training:
            mel = self.spec_augment(mel)
        
        speaker = torch.FloatTensor([speaker]).long()
        return mel, speaker
```


**5.Conformerå®ç°æ¶æ„å›¾**

```mermaid
graph TD
    A[Input Mel-Spectrogram<br/>Shape: batchÃ—timeÃ—40] --> B[Prenet<br/>Linear: 40â†’160]
    
    B --> C[Conformer Encoder<br/>n_layers=2]
    
    C --> D{Conformer Layer}
    
    D --> E[FFN 1<br/>Scale: 0.5 + Residual]
    E --> F{convolution_first?}
    
    F -->|No Default| G[Multi-Head Attention<br/>heads=4, d_model=160]
    F -->|Yes Optional| H[Convolution Module]
    
    G --> H
    H --> I[FFN 2<br/>Scale: 0.5 + Residual]
    I --> J[Final Layer Norm]
    
    J --> K{More Layers?}
    K -->|Yes| D
    K -->|No| L[Pooling Strategy]
    
    L --> M{use_sap?}
    M -->|True| N[Self-Attention Pooling<br/>Learnable Weights]
    M -->|False| O[Mean Pooling<br/>Simple Average]
    
    N --> P[Prediction Layers]
    O --> P
    
    P --> Q[BatchNorm1d<br/>d_model=160]
    Q --> R[Linear: 160â†’160]
    R --> S[ReLU + Dropout]
    S --> T[Linear: 160â†’n_speakers]
    T --> U[Output Logits]

    subgraph Conv [" Convolution Module Details "]
        CV1[Layer Norm] --> CV2[Conv1d: 160â†’320<br/>kernel=1]
        CV2 --> CV3[GLU: 320â†’160<br/>Gated Linear Unit]
        CV3 --> CV4[DepthwiseConv1d<br/>kernel=31, groups=160]
        CV4 --> CV5[BatchNorm1d /<br/>GroupNorm optional]
        CV5 --> CV6[SiLU Activation]
        CV6 --> CV7[Conv1d: 160â†’160<br/>kernel=1]
        CV7 --> CV8[Dropout + Residual]
    end

    subgraph FFN [" Feed Forward Network "]
        FF1[Layer Norm] --> FF2[Linear: 160â†’320<br/>ffn_dim=2Ã—d_model]
        FF2 --> FF3[SiLU Activation]
        FF3 --> FF4[Dropout]
        FF4 --> FF5[Linear: 320â†’160]
        FF5 --> FF6[Dropout]
    end

    subgraph MHSA [" Multi-Head Self-Attention "]
        MH1[Layer Norm] --> MH2[MultiheadAttention<br/>4 heads Ã— 40 dim each]
        MH2 --> MH3[Attention Dropout]
        MH3 --> MH4[Residual Connection]
    end

    style A fill:#e3f2fd,stroke:#1976d2
    style U fill:#e8f5e8,stroke:#388e3c
    style D fill:#fff8e1,stroke:#f57f17
    style Conv fill:#fce4ec,stroke:#c2185b
    style FFN fill:#f3e5f5,stroke:#7b1fa2
    style MHSA fill:#e8eaf6,stroke:#303f9f

```


### æˆç»©
![image1](./img/hw4_img1.png)