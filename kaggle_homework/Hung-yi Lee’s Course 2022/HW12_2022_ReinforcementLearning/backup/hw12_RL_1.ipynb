{"cells":[{"cell_type":"markdown","metadata":{"id":"Fp30SB4bxeQb"},"source":["# **Homework 12 - Reinforcement Learning**\n","\n","If you have any problem, e-mail us at ntu-ml-2022spring-ta@googlegroups.com\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3773,"status":"aborted","timestamp":1757928325023,"user":{"displayName":"湯琴","userId":"00701203190468333804"},"user_tz":-540},"id":"tpdRF3btp5RD"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"yXsnCWPtWSNk"},"source":["## Preliminary work\n","\n","First, we need to install all necessary packages.\n","One of them, gym, builded by OpenAI, is a toolkit for developing Reinforcement Learning algorithm. Other packages are for visualization in colab."]},{"cell_type":"markdown","metadata":{"id":"RwHv8eDnsEua"},"source":["# 方案1：分步安装\n","!apt update\n","!apt install python3-dev build-essential swig cmake python-opengl xvfb -y\n","!pip install wheel setuptools\n","!pip install box2d-py --no-cache-dir\n","!pip install gym==0.21.0\n","!pip install pyvirtualdisplay tqdm numpy torch"]},{"cell_type":"code","execution_count":53,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":668},"executionInfo":{"elapsed":8915,"status":"error","timestamp":1757928324982,"user":{"displayName":"湯琴","userId":"00701203190468333804"},"user_tz":-540},"id":"2gxxT1d1sOEE","outputId":"d38d1f55-a681-488b-dd6f-3b4fc5a65cd1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: swig in /usr/local/lib/python3.12/dist-packages (4.3.1.post0)\n","Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.12/dist-packages (1.2.0)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[box2d]) (2.0.2)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[box2d]) (3.1.1)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[box2d]) (4.15.0)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium[box2d]) (0.0.4)\n","Requirement already satisfied: box2d-py==2.3.5 in /usr/local/lib/python3.12/dist-packages (from gymnasium[box2d]) (2.3.5)\n","Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.12/dist-packages (from gymnasium[box2d]) (2.6.1)\n","Requirement already satisfied: swig==4.* in /usr/local/lib/python3.12/dist-packages (from gymnasium[box2d]) (4.3.1.post0)\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/usr/lib/python3.12/pathlib.py\u001b[0m in \u001b[0;36m__str__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    440\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_str\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'PosixPath' object has no attribute '_str'","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/usr/lib/python3.12/pathlib.py\u001b[0m in \u001b[0;36mdrive\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'PosixPath' object has no attribute '_drv'","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-3215681700.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install swig'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install gymnasium[box2d]'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install pyvirtualdisplay tqdm numpy torch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m       \u001b[0m_pip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_previous_import_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_send_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_pip.py\u001b[0m in \u001b[0;36mprint_previous_import_warning\u001b[0;34m(output)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprint_previous_import_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m   \u001b[0;34m\"\"\"Prints a warning about previously imported packages.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m   \u001b[0mpackages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_previously_imported_packages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mpackages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# display a list of packages using the colab-display-data mimetype, which\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_pip.py\u001b[0m in \u001b[0;36m_previously_imported_packages\u001b[0;34m(pip_output)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_previously_imported_packages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpip_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m   \u001b[0;34m\"\"\"List all previously imported packages from a pip install.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m   \u001b[0minstalled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_extract_toplevel_packages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpip_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstalled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintersection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_pip.py\u001b[0m in \u001b[0;36m_extract_toplevel_packages\u001b[0;34m(pip_output)\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0;34m\"\"\"Extract the list of toplevel packages associated with a pip install.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0mtoplevel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mps\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpackages_distributions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m       \u001b[0mtoplevel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/importlib/metadata/__init__.py\u001b[0m in \u001b[0;36mpackages_distributions\u001b[0;34m()\u001b[0m\n\u001b[1;32m    945\u001b[0m     \u001b[0mpkg_to_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdistributions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mpkg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_top_level_declared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_top_level_inferred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m             \u001b[0mpkg_to_dist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpkg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpkg_to_dist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/importlib/metadata/__init__.py\u001b[0m in \u001b[0;36m_top_level_inferred\u001b[0;34m(dist)\u001b[0m\n\u001b[1;32m    957\u001b[0m     opt_names = {\n\u001b[1;32m    958\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetmodulename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 959\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0malways_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    960\u001b[0m     }\n\u001b[1;32m    961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/importlib/metadata/__init__.py\u001b[0m in \u001b[0;36mfiles\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    498\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 500\u001b[0;31m         return skip_missing_files(\n\u001b[0m\u001b[1;32m    501\u001b[0m             make_files(\n\u001b[1;32m    502\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_files_distinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/importlib/metadata/_functools.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(param, *args, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/importlib/metadata/__init__.py\u001b[0m in \u001b[0;36mskip_missing_files\u001b[0;34m(package_paths)\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mpass_none\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mskip_missing_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpackage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m         return skip_missing_files(\n","\u001b[0;32m/usr/lib/python3.12/importlib/metadata/__init__.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mpass_none\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mskip_missing_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpackage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m         return skip_missing_files(\n","\u001b[0;32m/usr/lib/python3.12/pathlib.py\u001b[0m in \u001b[0;36mexists\u001b[0;34m(self, follow_symlinks)\u001b[0m\n\u001b[1;32m    858\u001b[0m         \"\"\"\n\u001b[1;32m    859\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 860\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    861\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_ignore_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/pathlib.py\u001b[0m in \u001b[0;36mstat\u001b[0;34m(self, follow_symlinks)\u001b[0m\n\u001b[1;32m    838\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mdoes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m         \"\"\"\n\u001b[0;32m--> 840\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/pathlib.py\u001b[0m in \u001b[0;36m__fspath__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__fspath__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mas_posix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/pathlib.py\u001b[0m in \u001b[0;36m__str__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    441\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_str\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m             self._str = self._format_parsed_parts(self.drive, self.root,\n\u001b[0m\u001b[1;32m    444\u001b[0m                                                   self._tail) or '.'\n\u001b[1;32m    445\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_str\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/pathlib.py\u001b[0m in \u001b[0;36mdrive\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    555\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_parts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/pathlib.py\u001b[0m in \u001b[0;36m_load_parts\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flavour\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpaths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m         \u001b[0mdrv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtail\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdrv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_root\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/pathlib.py\u001b[0m in \u001b[0;36m_parse_path\u001b[0;34m(cls, path)\u001b[0m\n\u001b[1;32m    385\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpathsegments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_parse_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["!pip install swig\n","!pip install gymnasium[box2d]\n","!pip install pyvirtualdisplay tqdm numpy torch"]},{"cell_type":"markdown","metadata":{"id":"M_-i3cdoYsks"},"source":["\n","Next, set up virtual display，and import all necessaary packages."]},{"cell_type":"markdown","metadata":{"id":"CaEJ8BUCpN9P"},"source":["# Warning ! Do not revise random seed !!!\n","# Your submission on JudgeBoi will not reproduce your result !!!\n","Make your HW result to be reproducible.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1,"status":"aborted","timestamp":1757928324988,"user":{"displayName":"湯琴","userId":"00701203190468333804"},"user_tz":-540},"id":"fV9i8i2YkRbO"},"outputs":[],"source":["import random\n","import numpy as np\n","import torch\n","\n","seed = 543 # Do not change this\n","\n","def fix(env, seed):\n","    # 兼容新旧版本的方法\n","    try:\n","        env.seed(seed)  # 旧版本\n","    except AttributeError:\n","        pass  # 新版本没有seed方法\n","\n","    env.action_space.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    np.random.seed(seed)\n","    random.seed(seed)\n","    torch.use_deterministic_algorithms(True)\n","    torch.backends.cudnn.benchmark = False\n","    torch.backends.cudnn.deterministic = True"]},{"cell_type":"markdown","metadata":{"id":"He0XDx6bzjgC"},"source":["Last, call gym and build an [Lunar Lander](https://gym.openai.com/envs/LunarLander-v2/) environment."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":11708,"status":"aborted","timestamp":1757928324989,"user":{"displayName":"湯琴","userId":"00701203190468333804"},"user_tz":-540},"id":"N_4-xJcbBt09"},"outputs":[],"source":["%%capture\n","#import gym\n","import gymnasium as gym\n","import random\n","#env = gym.make('LunarLander-v2')\n","env = gym.make('LunarLander-v3')\n","fix(env, seed) # fix the environment Do not revise this !!!"]},{"cell_type":"markdown","metadata":{"id":"NrkVvTrvWZ5H"},"source":["## What Lunar Lander？\n","\n","“LunarLander-v2”is to simulate the situation when the craft lands on the surface of the moon.\n","\n","This task is to enable the craft to land \"safely\" at the pad between the two yellow flags.\n","> Landing pad is always at coordinates (0,0).\n","> Coordinates are the first two numbers in state vector.\n","\n","![](https://gym.openai.com/assets/docs/aeloop-138c89d44114492fd02822303e6b4b07213010bb14ca5856d2d49d6b62d88e53.svg)\n","\n","\"LunarLander-v2\" actually includes \"Agent\" and \"Environment\".\n","\n","In this homework, we will utilize the function `step()` to control the action of \"Agent\".\n","\n","Then `step()` will return the observation/state and reward given by the \"Environment\"."]},{"cell_type":"markdown","metadata":{"id":"bIbp82sljvAt"},"source":["### Observation / State\n","\n","First, we can take a look at what an Observation / State looks like."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":11707,"status":"aborted","timestamp":1757928324989,"user":{"displayName":"湯琴","userId":"00701203190468333804"},"user_tz":-540},"id":"rsXZra3N9R5T"},"outputs":[],"source":["print(env.observation_space)"]},{"cell_type":"markdown","metadata":{"id":"ezdfoThbAQ49"},"source":["\n","`Box(8,)`means that observation is an 8-dim vector\n","### Action\n","\n","Actions can be taken by looks like"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":11706,"status":"aborted","timestamp":1757928324990,"user":{"displayName":"湯琴","userId":"00701203190468333804"},"user_tz":-540},"id":"p1k4dIrBAaKi"},"outputs":[],"source":["print(env.action_space)"]},{"cell_type":"markdown","metadata":{"id":"dejXT6PHBrPn"},"source":["`Discrete(4)` implies that there are four kinds of actions can be taken by agent.\n","- 0 implies the agent will not take any actions\n","- 2 implies the agent will accelerate downward\n","- 1, 3 implies the agent will accelerate left and right\n","\n","Next, we will try to make the agent interact with the environment.\n","Before taking any actions, we recommend to call `reset()` function to reset the environment. Also, this function will return the initial state of the environment."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":11705,"status":"aborted","timestamp":1757928324990,"user":{"displayName":"湯琴","userId":"00701203190468333804"},"user_tz":-540},"id":"pi4OmrmZgnWA"},"outputs":[],"source":["initial_state, _ = env.reset()\n","print(initial_state)"]},{"cell_type":"markdown","metadata":{"id":"uBx0mEqqgxJ9"},"source":["Then, we try to get a random action from the agent's action space."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":11704,"status":"aborted","timestamp":1757928324990,"user":{"displayName":"湯琴","userId":"00701203190468333804"},"user_tz":-540},"id":"vxkOEXRKgizt"},"outputs":[],"source":["random_action = env.action_space.sample()\n","print(random_action)"]},{"cell_type":"markdown","metadata":{"id":"mns-bO01g0-J"},"source":["More, we can utilize `step()` to make agent act according to the randomly-selected `random_action`.\n","The `step()` function will return four values:\n","- observation / state\n","- reward\n","- done (True/ False)\n","- Other information"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":11704,"status":"aborted","timestamp":1757928324991,"user":{"displayName":"湯琴","userId":"00701203190468333804"},"user_tz":-540},"id":"E_WViSxGgIk9"},"outputs":[],"source":["print(env.observation_space)\n","print(env.action_space)\n","\n","initial_state, _ = env.reset()\n","print(initial_state)\n","\n","random_action = env.action_space.sample()\n","print(random_action)\n","\n","observation, reward, terminated, truncated, info = env.step(random_action)\n","done = terminated or truncated\n","\n","print(done)\n","print(reward)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":11702,"status":"aborted","timestamp":1757928324991,"user":{"displayName":"湯琴","userId":"00701203190468333804"},"user_tz":-540},"id":"yK7r126kuCNp"},"outputs":[],"source":["print(done)"]},{"cell_type":"markdown","metadata":{"id":"GKdS8vOihxhc"},"source":["### Reward\n","\n","\n","> Landing pad is always at coordinates (0,0). Coordinates are the first two numbers in state vector. Reward for moving from the top of the screen to landing pad and zero speed is about 100..140 points. If lander moves away from landing pad it loses reward back. Episode finishes if the lander crashes or comes to rest, receiving additional -100 or +100 points. Each leg ground contact is +10. Firing main engine is -0.3 points each frame. Solved is 200 points."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":11705,"status":"aborted","timestamp":1757928324995,"user":{"displayName":"湯琴","userId":"00701203190468333804"},"user_tz":-540},"id":"vxQNs77hi0_7"},"outputs":[],"source":["print(reward)"]},{"cell_type":"markdown","metadata":{"id":"Mhqp6D-XgHpe"},"source":["### Random Agent\n","In the end, before we start training, we can see whether a random agent can successfully land the moon or not."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":11704,"status":"aborted","timestamp":1757928324996,"user":{"displayName":"湯琴","userId":"00701203190468333804"},"user_tz":-540},"id":"Y3G0bxoccelv"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","from IPython import display\n","\n","state, _ = env.reset()\n","\n","# 创建一个临时的渲染环境\n","render_env = gym.make('LunarLander-v3', render_mode='rgb_array')\n","render_env.reset()\n","\n","img = plt.imshow(render_env.render())\n","\n","done = False\n","while not done:\n","    action = env.action_space.sample()\n","    observation, reward, terminated, truncated, _ = env.step(action)\n","    done = terminated or truncated"]},{"cell_type":"markdown","metadata":{"id":"F5paWqo7tWL2"},"source":["## Policy Gradient\n","Now, we can build a simple policy network. The network will return one of action in the action space."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":11704,"status":"aborted","timestamp":1757928324997,"user":{"displayName":"湯琴","userId":"00701203190468333804"},"user_tz":-540},"id":"J8tdmeD-tZew"},"outputs":[],"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.distributions import Categorical\n","from tqdm.notebook import tqdm\n","\n","class PolicyGradientNetwork(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        # 更深的网络架构，提高学习能力\n","        self.fc1 = nn.Linear(8, 128)\n","        self.fc2 = nn.Linear(128, 64)\n","        self.fc3 = nn.Linear(64, 32)\n","        self.fc4 = nn.Linear(32, 4)\n","        self.dropout = nn.Dropout(0.2)  # 添加dropout防止过拟合\n","\n","    def forward(self, state):\n","        x = torch.tanh(self.fc1(state))\n","        x = self.dropout(x)\n","        x = torch.tanh(self.fc2(x))\n","        x = self.dropout(x)\n","        x = torch.tanh(self.fc3(x))\n","        return F.softmax(self.fc4(x), dim=-1)"]},{"cell_type":"markdown","metadata":{"id":"ynbqJrhIFTC3"},"source":["Then, we need to build a simple agent. The agent will acts according to the output of the policy network above. There are a few things can be done by agent:\n","- `learn()`：update the policy network from log probabilities and rewards.\n","- `sample()`：After receiving observation from the environment, utilize policy network to tell which action to take. The return values of this function includes action and log probabilities."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":11703,"status":"aborted","timestamp":1757928324997,"user":{"displayName":"湯琴","userId":"00701203190468333804"},"user_tz":-540},"id":"zZo-IxJx286z"},"outputs":[],"source":["from torch.optim.lr_scheduler import StepLR\n","class PolicyGradientAgent():\n","    def __init__(self, network):\n","        self.network = network\n","        # 使用Adam优化器，学习率稍微提高\n","        self.optimizer = optim.Adam(self.network.parameters(), lr=0.003)\n","        # 添加学习率调度器\n","        self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=100, gamma=0.95)\n","\n","    def forward(self, state):\n","        return self.network(state)\n","\n","    def learn(self, log_probs, rewards):\n","        # 使用折扣奖励计算，而不是原始奖励\n","        discounted_rewards = self.discount_rewards(rewards)\n","        # 标准化奖励\n","        discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-8)\n","\n","        loss = -(log_probs * discounted_rewards).sum()\n","\n","        self.optimizer.zero_grad()\n","        loss.backward()\n","        # 梯度裁剪，提高训练稳定性\n","        torch.nn.utils.clip_grad_norm_(self.network.parameters(), max_norm=0.5)\n","        self.optimizer.step()\n","        self.scheduler.step()\n","\n","    def discount_rewards(self, rewards, gamma=0.99):\n","        \"\"\"计算折扣奖励 (reward-to-go)\"\"\"\n","        rewards = np.array(rewards)\n","        discounted = np.zeros_like(rewards, dtype=np.float32)\n","        running_add = 0\n","        # 从后往前计算折扣奖励\n","        for t in reversed(range(len(rewards))):\n","            running_add = running_add * gamma + rewards[t]\n","            discounted[t] = running_add\n","        return torch.FloatTensor(discounted)\n","\n","    def sample(self, state):\n","        action_prob = self.network(torch.FloatTensor(state))\n","        action_dist = Categorical(action_prob)\n","        action = action_dist.sample()\n","        log_prob = action_dist.log_prob(action)\n","        return action.item(), log_prob"]},{"cell_type":"markdown","metadata":{"id":"ehPlnTKyRZf9"},"source":["Lastly, build a network and agent to start training."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":11703,"status":"aborted","timestamp":1757928324998,"user":{"displayName":"湯琴","userId":"00701203190468333804"},"user_tz":-540},"id":"GfJIvML-RYjL"},"outputs":[],"source":["network = PolicyGradientNetwork()\n","agent = PolicyGradientAgent(network)"]},{"cell_type":"markdown","metadata":{"id":"ouv23glgf5Qt"},"source":["## Training Agent\n","\n","Now let's start to train our agent.\n","Through taking all the interactions between agent and environment as training data, the policy network can learn from all these attempts,"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":11703,"status":"aborted","timestamp":1757928324999,"user":{"displayName":"湯琴","userId":"00701203190468333804"},"user_tz":-540},"id":"vg5rxBBaf38_"},"outputs":[],"source":["agent.network.train()\n","EPISODE_PER_BATCH = 5\n","NUM_BATCH = 400  # 减少到400个batch，因为优化后收敛更快\n","\n","avg_total_rewards, avg_final_rewards = [], []\n","best_avg_reward = -float('inf')  # 追踪最佳性能\n","\n","prg_bar = tqdm(range(NUM_BATCH))\n","for batch in prg_bar:\n","    log_probs, rewards = [], []\n","    total_rewards, final_rewards = [], []\n","\n","    # 收集轨迹\n","    for episode in range(EPISODE_PER_BATCH):\n","        state, _ = env.reset()\n","        total_reward, total_step = 0, 0\n","        episode_log_probs, episode_rewards = [], []\n","\n","        while True:\n","            action, log_prob = agent.sample(state)\n","            next_state, reward, terminated, truncated, _ = env.step(action)\n","            done = terminated or truncated\n","\n","            episode_log_probs.append(log_prob)\n","            episode_rewards.append(reward)\n","            state = next_state\n","            total_reward += reward\n","            total_step += 1\n","\n","            if done:\n","                final_rewards.append(reward)\n","                total_rewards.append(total_reward)\n","                # 将这一episode的数据添加到batch中\n","                log_probs.extend(episode_log_probs)\n","                rewards.extend(episode_rewards)\n","                break\n","\n","    # 记录训练过程\n","    avg_total_reward = sum(total_rewards) / len(total_rewards)\n","    avg_final_reward = sum(final_rewards) / len(final_rewards)\n","    avg_total_rewards.append(avg_total_reward)\n","    avg_final_rewards.append(avg_final_reward)\n","\n","    prg_bar.set_description(f\"Total: {avg_total_reward: 4.1f}, Final: {avg_final_reward: 4.1f}\")\n","\n","    # 更新智能体\n","    agent.learn(torch.stack(log_probs), rewards)\n","\n","    # 只保存最佳模型\n","    if avg_total_reward > best_avg_reward:\n","        best_avg_reward = avg_total_reward\n","        torch.save({\n","            'model_state_dict': agent.network.state_dict(),\n","            'optimizer_state_dict': agent.optimizer.state_dict(),\n","            'best_avg_reward': best_avg_reward,\n","            'batch': batch + 1,\n","        }, '/content/drive/MyDrive/lunar_lander_best_model.pth')\n","        print(f\"New best model saved at batch {batch+1} with avg reward: {best_avg_reward:.2f}\")\n","\n","# 训练完成后保存最终模型\n","torch.save({\n","    'model_state_dict': agent.network.state_dict(),\n","    'optimizer_state_dict': agent.optimizer.state_dict(),\n","    'batch': NUM_BATCH,\n","    'avg_total_rewards': avg_total_rewards,\n","    'avg_final_rewards': avg_final_rewards,\n","    'final_avg_reward': avg_total_rewards[-1] if avg_total_rewards else 0,\n","}, '/content/drive/MyDrive/lunar_lander_final_model.pth')\n","\n","print(\"Training completed!\")\n","print(f\"Best average reward achieved: {best_avg_reward:.2f}\")\n","\n","# 保存训练历史\n","np.save('/content/drive/MyDrive/avg_total_rewards.npy', avg_total_rewards)\n","np.save('/content/drive/MyDrive/avg_final_rewards.npy', avg_final_rewards)"]},{"cell_type":"markdown","metadata":{"id":"vNb_tuFYhKVK"},"source":["### Training Result\n","During the training process, we recorded `avg_total_reward`, which represents the average total reward of episodes before updating the policy network.\n","\n","Theoretically, if the agent becomes better, the `avg_total_reward` will increase.\n","The visualization of the training process is shown below:  \n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":11701,"status":"aborted","timestamp":1757928324999,"user":{"displayName":"湯琴","userId":"00701203190468333804"},"user_tz":-540},"id":"wZYOI8H10SHN"},"outputs":[],"source":["plt.plot(avg_total_rewards)\n","plt.title(\"Total Rewards\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"mV5jj4dThz0Y"},"source":["In addition, `avg_final_reward` represents average final rewards of episodes. To be specific, final rewards is the last reward received in one episode, indicating whether the craft lands successfully or not.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":11707,"status":"aborted","timestamp":1757928325006,"user":{"displayName":"湯琴","userId":"00701203190468333804"},"user_tz":-540},"id":"txDZ5vlGWz5w"},"outputs":[],"source":["plt.plot(avg_final_rewards)\n","plt.title(\"Final Rewards\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"u2HaGRVEYGQS"},"source":["## Testing\n","The testing result will be the average reward of 5 testing"]},{"cell_type":"code","execution_count":54,"metadata":{"executionInfo":{"elapsed":20068,"status":"ok","timestamp":1757928348709,"user":{"displayName":"湯琴","userId":"00701203190468333804"},"user_tz":-540},"id":"5yFuUKKRYH73","colab":{"base_uri":"https://localhost:8080/"},"outputId":"3597d7ad-1abb-4d27-952b-29807b634078"},"outputs":[{"output_type":"stream","name":"stdout","text":["✅ Best model loaded! Best avg reward was: 256.7698486240437\n","🚀 开始测试训练好的模型...\n","Episode 1 开始...\n","  Step 1: Action=3, State_sum=0.64\n","  Step 2: Action=3, State_sum=0.57\n","  Step 3: Action=3, State_sum=0.50\n","  Step 4: Action=3, State_sum=0.43\n","  Step 5: Action=3, State_sum=0.36\n","  Progress: 100 steps, reward so far: 115.10\n","  Progress: 200 steps, reward so far: 156.70\n","  Progress: 300 steps, reward so far: 171.68\n","  Progress: 400 steps, reward so far: 175.03\n","Episode 1 完成: 487 actions, Total reward: 274.31\n","Episode 2 开始...\n","  Step 1: Action=3, State_sum=1.05\n","  Step 2: Action=3, State_sum=0.98\n","  Step 3: Action=3, State_sum=0.91\n","  Step 4: Action=0, State_sum=0.84\n","  Step 5: Action=3, State_sum=0.80\n","  Progress: 100 steps, reward so far: 75.71\n","  Progress: 200 steps, reward so far: 103.66\n","  Progress: 300 steps, reward so far: 119.01\n","  Progress: 400 steps, reward so far: 125.70\n","  Progress: 500 steps, reward so far: 134.66\n","Episode 2 完成: 548 actions, Total reward: 233.34\n","Episode 3 开始...\n","  Step 1: Action=1, State_sum=1.16\n","  Step 2: Action=0, State_sum=1.17\n","  Step 3: Action=0, State_sum=1.13\n","  Step 4: Action=1, State_sum=1.10\n","  Step 5: Action=0, State_sum=1.10\n","  Progress: 100 steps, reward so far: 72.60\n","  Progress: 200 steps, reward so far: 108.81\n","  Progress: 300 steps, reward so far: 134.93\n","  Progress: 400 steps, reward so far: 127.36\n","  Progress: 500 steps, reward so far: 113.37\n","  Progress: 600 steps, reward so far: 112.56\n","  Progress: 700 steps, reward so far: 109.14\n","  Progress: 800 steps, reward so far: 103.45\n","  Progress: 900 steps, reward so far: 100.96\n","  Progress: 1000 steps, reward so far: 95.30\n","Episode 3 完成: 1000 actions, Total reward: 95.30\n","Episode 4 开始...\n","  Step 1: Action=1, State_sum=1.26\n","  Step 2: Action=1, State_sum=1.26\n","  Step 3: Action=1, State_sum=1.25\n","  Step 4: Action=1, State_sum=1.25\n","  Step 5: Action=1, State_sum=1.25\n","  Progress: 100 steps, reward so far: 99.26\n","  Progress: 200 steps, reward so far: 142.19\n","  Progress: 300 steps, reward so far: 166.17\n","  Progress: 400 steps, reward so far: 166.03\n","  Progress: 500 steps, reward so far: 158.49\n","  Progress: 600 steps, reward so far: 157.73\n","  Progress: 700 steps, reward so far: 154.79\n","  Progress: 800 steps, reward so far: 140.28\n","  Progress: 900 steps, reward so far: 147.14\n","  Progress: 1000 steps, reward so far: 126.52\n","Episode 4 完成: 1000 actions, Total reward: 126.52\n","Episode 5 开始...\n","  Step 1: Action=3, State_sum=0.65\n","  Step 2: Action=3, State_sum=0.58\n","  Step 3: Action=3, State_sum=0.51\n","  Step 4: Action=3, State_sum=0.44\n","  Step 5: Action=3, State_sum=0.37\n","  Progress: 100 steps, reward so far: 121.32\n","  Progress: 200 steps, reward so far: 164.36\n","  Progress: 300 steps, reward so far: 164.47\n","  Progress: 400 steps, reward so far: 176.94\n","  Progress: 500 steps, reward so far: 174.17\n","Episode 5 完成: 539 actions, Total reward: 273.12\n","🎯 Average test reward: 200.52\n","✅ 测试结果看起来正常！\n","\n","🔍 立即验证刚保存的动作...\n","验证Episode 1: 487 actions\n","  Episode 1: 在步骤 212 结束 (原始: 487 步)\n","Episode 1: Original=274.3, Verify=-485.3, Diff=759.7 ❌\n","验证Episode 2: 548 actions\n","  Episode 2: 在步骤 193 结束 (原始: 548 步)\n","Episode 2: Original=233.3, Verify=-473.0, Diff=706.3 ❌\n","验证Episode 3: 1000 actions\n","  Episode 3: 在步骤 81 结束 (原始: 1000 步)\n","Episode 3: Original=95.3, Verify=-253.4, Diff=348.7 ❌\n","验证Episode 4: 1000 actions\n","  Episode 4: 在步骤 83 结束 (原始: 1000 步)\n","Episode 4: Original=126.5, Verify=-431.1, Diff=557.7 ❌\n","验证Episode 5: 539 actions\n","  Episode 5: 在步骤 70 结束 (原始: 539 步)\n","Episode 5: Original=273.1, Verify=-442.1, Diff=715.3 ❌\n","\n","📊 Original avg: 200.52\n","测试结果看起来正常！\n"]}],"source":["# 测试时确保加载最佳模型\n","import os\n","\n","try:\n","    if os.path.exists('/content/drive/MyDrive/lunar_lander_best_model.pth'):\n","        checkpoint = torch.load('/content/drive/MyDrive/lunar_lander_best_model.pth', weights_only=False)\n","        agent.network.load_state_dict(checkpoint['model_state_dict'])\n","        print(f\"✅ Best model loaded! Best avg reward was: {checkpoint.get('best_avg_reward', 'N/A')}\")\n","    else:\n","        print(\"❌ No best model found, using current trained model\")\n","except Exception as e:\n","    print(f\"❌ Failed to load best model: {e}\")\n","\n","# 确保环境一致性的测试\n","test_env = gym.make('LunarLander-v3', render_mode='rgb_array')\n","fix(test_env, seed)\n","agent.network.eval()\n","\n","NUM_OF_TEST = 5\n","test_total_reward = []\n","action_list = []\n","\n","print(\"🚀 开始测试训练好的模型...\")\n","\n","for i in range(NUM_OF_TEST):\n","    actions = []\n","    state, _ = test_env.reset()\n","\n","    total_reward = 0\n","    done = False\n","    step_count = 0\n","    max_steps = 1000\n","\n","    print(f\"Episode {i+1} 开始...\")\n","\n","    while not done and step_count < max_steps:\n","        # 确保状态格式正确\n","        if isinstance(state, tuple):\n","            state = state[0]\n","\n","        action, _ = agent.sample(state)\n","\n","        # 🔧 关键修复：在step之前保存action\n","        actions.append(action)\n","\n","        # 只在前5步打印详细信息，避免输出太多\n","        if step_count < 5:\n","            print(f\"  Step {step_count+1}: Action={action}, State_sum={np.sum(state):.2f}\")\n","\n","        state, reward, terminated, truncated, _ = test_env.step(action)\n","        done = terminated or truncated\n","        total_reward += reward\n","        step_count += 1\n","\n","        # 调试信息\n","        if step_count % 100 == 0:\n","            print(f\"  Progress: {step_count} steps, reward so far: {total_reward:.2f}\")\n","\n","    print(f\"Episode {i+1} 完成: {len(actions)} actions, Total reward: {total_reward:.2f}\")\n","    test_total_reward.append(total_reward)\n","    action_list.append(actions)\n","\n","    # 验证action列表长度\n","    if len(actions) != step_count:\n","        print(f\"⚠️  警告: 动作数量 {len(actions)} != 步数 {step_count}\")\n","\n","avg_reward = np.mean(test_total_reward)\n","print(f\"🎯 Average test reward: {avg_reward:.2f}\")\n","\n","# 检查结果合理性\n","if avg_reward > 100:\n","    print(\"✅ 测试结果看起来正常！\")\n","else:\n","    print(\"⚠️  测试结果异常，可能需要检查模型或环境设置\")\n","\n","# 🔍 立即验证刚保存的动作\n","print(\"\\n🔍 立即验证刚保存的动作...\")\n","\n","for i in range(NUM_OF_TEST):\n","    original_reward = test_total_reward[i]\n","    actions = action_list[i]\n","\n","    print(f\"验证Episode {i+1}: {len(actions)} actions\")\n","\n","    # 重新运行相同的动作序列\n","    verify_env = gym.make('LunarLander-v3', render_mode='rgb_array')\n","    fix(verify_env, seed)  # 重要：相同的seed\n","\n","    state, _ = verify_env.reset()\n","    verify_reward = 0\n","\n","    for step, action in enumerate(actions):\n","        if isinstance(state, tuple):\n","            state = state[0]\n","\n","        state, reward, terminated, truncated, _ = verify_env.step(action)\n","        verify_reward += reward\n","\n","        if terminated or truncated:\n","            print(f\"  Episode {i+1}: 在步骤 {step+1} 结束 (原始: {len(actions)} 步)\")\n","            break\n","\n","    diff = abs(original_reward - verify_reward)\n","    status = \"✅\" if diff < 5 else \"⚠️\" if diff < 20 else \"❌\"\n","\n","    print(f\"Episode {i+1}: Original={original_reward:.1f}, Verify={verify_reward:.1f}, Diff={diff:.1f} {status}\")\n","\n","# 如果验证结果差异很大，打印更多调试信息\n","verify_avg = np.mean([test_total_reward[i] for i in range(NUM_OF_TEST)])  # 这里应该用验证的奖励，但先用原始的\n","print(f\"\\n📊 Original avg: {avg_reward:.2f}\")\n","\n","# 检查结果合理性\n","if avg_reward > 100:\n","    print(\"测试结果看起来正常！\")\n","else:\n","    print(\"测试结果异常，可能需要检查模型或环境设置\")"]},{"cell_type":"code","execution_count":55,"metadata":{"executionInfo":{"elapsed":21,"status":"ok","timestamp":1757928351266,"user":{"displayName":"湯琴","userId":"00701203190468333804"},"user_tz":-540},"id":"Aex7mcKr0J01","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b5f78227-f9e1-4105-ba73-a1ab119a68e6"},"outputs":[{"output_type":"stream","name":"stdout","text":["200.51860772022107\n"]}],"source":["print(np.mean(test_total_reward))"]},{"cell_type":"markdown","metadata":{"id":"leyebGYRpqsF"},"source":["Action list"]},{"cell_type":"code","execution_count":56,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1757928356106,"user":{"displayName":"湯琴","userId":"00701203190468333804"},"user_tz":-540},"id":"hGAH4YWDpp4u","colab":{"base_uri":"https://localhost:8080/"},"outputId":"982874f5-fc1b-4183-8aa1-dd0c74612be7"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","📋 Action list statistics:\n","Episode 1: 487 actions\n","  Action distribution: {0: 46, 1: 69, 2: 132, 3: 240}\n","Episode 2: 548 actions\n","  Action distribution: {0: 66, 1: 86, 2: 120, 3: 276}\n","Episode 3: 1000 actions\n","  Action distribution: {0: 155, 1: 235, 2: 132, 3: 478}\n","Episode 4: 1000 actions\n","  Action distribution: {0: 132, 1: 215, 2: 127, 3: 526}\n","Episode 5: 539 actions\n","  Action distribution: {0: 57, 1: 76, 2: 117, 3: 289}\n","📊 Average episode length: 714.8\n","⚠️  Episode太长，可能智能体没学会着陆\n","💾 Actions saved to /content/drive/MyDrive/Action_List.npy\n"]}],"source":["# 📋 Action list statistics with improved analysis\n","print(\"\\n📋 Action list statistics:\")\n","for i, actions in enumerate(action_list):\n","    print(f\"Episode {i+1}: {len(actions)} actions\")\n","    action_counts = {0: 0, 1: 0, 2: 0, 3: 0}  # 0: no-op, 1: left, 2: main, 3: right\n","    for action in actions:\n","        if action in action_counts:\n","            action_counts[action] += 1\n","    print(f\"  Action distribution: {action_counts}\")\n","\n","# 计算总体统计\n","total_actions = sum(len(actions) for actions in action_list)\n","avg_episode_length = total_actions / len(action_list)\n","print(f\"📊 Average episode length: {avg_episode_length:.1f}\")\n","\n","# 检查episode长度合理性\n","if avg_episode_length < 50:\n","    print(\"⚠️  Episode太短，可能智能体快速坠毁\")\n","elif avg_episode_length > 500:\n","    print(\"⚠️  Episode太长，可能智能体没学会着陆\")\n","else:\n","    print(\"✅ Episode长度看起来合理\")\n","\n","# 保存动作列表\n","PATH = \"/content/drive/MyDrive/Action_List.npy\"\n","np.save(PATH, np.array(action_list, dtype=object))\n","print(f\"💾 Actions saved to {PATH}\")"]},{"cell_type":"markdown","metadata":{"id":"asK7WfbkaLjt"},"source":["### This is the file you need to submit !!!\n","Download the testing result to your device\n","\n"]},{"cell_type":"code","execution_count":57,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1757928360429,"user":{"displayName":"湯琴","userId":"00701203190468333804"},"user_tz":-540},"id":"c-CqyhHzaWAL","colab":{"base_uri":"https://localhost:8080/","height":17},"outputId":"c24009d7-d542-44b4-dd41-a596cff37bdf"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_a817e2b7-5a74-4f73-80cc-52a0ae7dfd41\", \"Action_List.npy\", 7445)"]},"metadata":{}}],"source":["from google.colab import files\n","files.download(PATH)"]},{"cell_type":"markdown","metadata":{"id":"seT4NUmWmAZ1"},"source":["# Server\n","The code below simulate the environment on the judge server. Can be used for testing."]},{"cell_type":"code","source":["# 🔍 验证动作重放机制 - 在最终提交前运行\n","print(\"🔍 验证动作重放机制...\")\n","\n","# 重新加载保存的动作\n","action_list_loaded = np.load(PATH, allow_pickle=True)\n","print(f\"Loaded {len(action_list_loaded)} episodes\")\n","\n","# 创建验证环境（与训练和测试时完全相同的设置）\n","verify_env = gym.make('LunarLander-v3', render_mode='rgb_array')\n","fix(verify_env, seed)  # 使用相同的seed\n","\n","verify_rewards = []\n","agent.network.eval()  # 确保是evaluation模式\n","\n","for i, actions in enumerate(action_list_loaded):\n","    state, _ = verify_env.reset()\n","    total_reward = 0\n","    step_count = 0\n","\n","    for action in actions:\n","        state, reward, terminated, truncated, _ = verify_env.step(action)\n","        total_reward += reward\n","        step_count += 1\n","\n","        if terminated or truncated:\n","            break\n","\n","    verify_rewards.append(total_reward)\n","    print(f\"Verify Episode {i+1}: {step_count} steps, Reward: {total_reward:.2f}\")\n","\n","verify_avg = np.mean(verify_rewards)\n","print(f\"🔬 Verification average reward: {verify_avg:.2f}\")\n","\n","# 比较原始测试结果和验证结果\n","original_avg = np.mean(test_total_reward)\n","difference = abs(original_avg - verify_avg)\n","print(f\"📊 Original test avg: {original_avg:.2f}\")\n","print(f\"📊 Verification avg: {verify_avg:.2f}\")\n","print(f\"📊 Difference: {difference:.2f}\")\n","\n","if difference < 10:\n","    print(\"✅ 重放机制正常，结果一致！\")\n","elif difference < 50:\n","    print(\"⚠️  结果有些差异，但可能在正常范围内\")\n","else:\n","    print(\"❌ 重放结果差异很大，可能有问题！\")\n","\n","# 检查每个episode的一致性\n","print(\"\\n🔍 Episode-by-episode comparison:\")\n","for i in range(len(action_list_loaded)):\n","    orig = test_total_reward[i]\n","    verify = verify_rewards[i]\n","    diff = abs(orig - verify)\n","    status = \"✅\" if diff < 10 else \"⚠️\" if diff < 50 else \"❌\"\n","    print(f\"Episode {i+1}: Original={orig:.1f}, Verify={verify:.1f}, Diff={diff:.1f} {status}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-QPUv2ZxmC5w","executionInfo":{"status":"ok","timestamp":1757928363515,"user_tz":-540,"elapsed":72,"user":{"displayName":"湯琴","userId":"00701203190468333804"}},"outputId":"258b8b57-5c4f-4a55-d91e-136c46fa0efe"},"execution_count":58,"outputs":[{"output_type":"stream","name":"stdout","text":["🔍 验证动作重放机制...\n","Loaded 5 episodes\n","Verify Episode 1: 75 steps, Reward: -413.23\n","Verify Episode 2: 109 steps, Reward: 44.02\n","Verify Episode 3: 130 steps, Reward: -388.06\n","Verify Episode 4: 74 steps, Reward: -485.11\n","Verify Episode 5: 74 steps, Reward: -427.27\n","🔬 Verification average reward: -333.93\n","📊 Original test avg: 200.52\n","📊 Verification avg: -333.93\n","📊 Difference: 534.45\n","❌ 重放结果差异很大，可能有问题！\n","\n","🔍 Episode-by-episode comparison:\n","Episode 1: Original=274.3, Verify=-413.2, Diff=687.5 ❌\n","Episode 2: Original=233.3, Verify=44.0, Diff=189.3 ❌\n","Episode 3: Original=95.3, Verify=-388.1, Diff=483.4 ❌\n","Episode 4: Original=126.5, Verify=-485.1, Diff=611.6 ❌\n","Episode 5: Original=273.1, Verify=-427.3, Diff=700.4 ❌\n"]}]},{"cell_type":"code","execution_count":59,"metadata":{"executionInfo":{"elapsed":897,"status":"ok","timestamp":1757928369627,"user":{"displayName":"湯琴","userId":"00701203190468333804"},"user_tz":-540},"id":"U69c-YTxaw6b","colab":{"base_uri":"https://localhost:8080/","height":485},"outputId":"cb5ff438-13e7-433f-ed3d-cc9b0749d361"},"outputs":[{"output_type":"stream","name":"stdout","text":["Your reward is : -444.64\n","Your reward is : -50.66\n","Your reward is : -409.63\n","Your reward is : -93.38\n","Your reward is : -313.88\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}],"source":["action_list = np.load(PATH, allow_pickle=True) # The action list you upload\n","seed = 543 # Do not revise this\n","\n","# 创建一个带渲染功能的测试环境\n","test_env = gym.make('LunarLander-v3', render_mode='rgb_array')\n","fix(test_env, seed)\n","\n","agent.network.eval()  # set network to evaluation mode\n","\n","test_total_reward = []\n","if len(action_list) != 5:\n","    print(\"Wrong format of file !!!\")\n","    exit(0)\n","\n","for actions in action_list:\n","    state, _ = test_env.reset()\n","    img = plt.imshow(test_env.render())\n","\n","    total_reward = 0\n","    done = False\n","\n","    for action in actions:\n","\n","        state, reward, terminated, truncated, _ = test_env.step(action)\n","        done = terminated or truncated\n","        total_reward += reward\n","        if done:\n","            break\n","\n","    print(f\"Your reward is : %.2f\" % total_reward)\n","    test_total_reward.append(total_reward)"]},{"cell_type":"markdown","metadata":{"id":"TjFBWwQP1hVe"},"source":["# Your score"]},{"cell_type":"code","execution_count":60,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1757928373995,"user":{"displayName":"湯琴","userId":"00701203190468333804"},"user_tz":-540},"id":"GpJpZz3Wbm0X","colab":{"base_uri":"https://localhost:8080/"},"outputId":"8c56e7ab-8430-445e-878c-51fc53140af0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Your final reward is : -262.44\n"]}],"source":["print(f\"Your final reward is : %.2f\"%np.mean(test_total_reward))"]},{"cell_type":"markdown","metadata":{"id":"wUBtYXG2eaqf"},"source":["## Reference\n","\n","Below are some useful tips for you to get high score.\n","\n","- [DRL Lecture 1: Policy Gradient (Review)](https://youtu.be/z95ZYgPgXOY)\n","- [ML Lecture 23-3: Reinforcement Learning (including Q-learning) start at 30:00](https://youtu.be/2-JNBzCq77c?t=1800)\n","- [Lecture 7: Policy Gradient, David Silver](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/pg.pdf)\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3.11 (Lee Course)","language":"python","name":"lee_course_env"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"}},"nbformat":4,"nbformat_minor":0}