{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15f582bd-45e9-4ea2-80fd-a99423565500",
   "metadata": {},
   "source": [
    "# Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f27bbb38-3729-44f8-a833-e55cf89c63a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b41a06-8de7-4e34-a21c-1c9edc59af51",
   "metadata": {},
   "source": [
    "```\n",
    "基础注意力机制 → Seq2Seq注意力 → 多头注意力 → Self-Attention → Transformer\n",
    "     ↑              ↑              ↑           ↑            ↑\n",
    "  点积/加性      用加性注意力     用点积注意力   自注意力    最终形态"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d254d33-6a25-470d-addd-610f6bb63414",
   "metadata": {},
   "source": [
    "Self-attention革命性的地方就在于打破了序列处理的限制，实现了真正的并行计算"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3db8a02-7b20-4215-b98a-4abe119792a8",
   "metadata": {},
   "source": [
    "```\n",
    "MultiHeadAttention ⊃ Multi-head Self-attention\n",
    "      通用框架        特殊应用"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ecb431-ab6d-462d-9c33-c9278f2e74cf",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "443d167d-55c0-4857-b1eb-95ebbc6603e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(d2l.Module): \n",
    "    \"\"\"Multi-head attention.\"\"\"\n",
    "    def __init__(self, num_hiddens, num_heads, dropout, bias=False, **kwargs):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads  # 专家数量（比如5个）\n",
    "        self.attention = d2l.DotProductAttention(dropout)  # 基础注意力计算器\n",
    "        \n",
    "        # 为每个专家准备4套工具：\n",
    "        self.W_q = nn.LazyLinear(num_hiddens, bias=bias)  # Query投影矩阵\n",
    "        self.W_k = nn.LazyLinear(num_hiddens, bias=bias)  # Key投影矩阵  \n",
    "        self.W_v = nn.LazyLinear(num_hiddens, bias=bias)  # Value投影矩阵\n",
    "        self.W_o = nn.LazyLinear(num_hiddens, bias=bias)  # 输出投影矩阵\n",
    "\n",
    "    def forward(self, queries, keys, values, valid_lens):\n",
    "        # 第1步：给每个专家分配专门的 Q、K、V\n",
    "        queries = self.transpose_qkv(self.W_q(queries))  # 分配查询\n",
    "        keys = self.transpose_qkv(self.W_k(keys))        # 分配键  \n",
    "        values = self.transpose_qkv(self.W_v(values))    # 分配值\n",
    "\n",
    "        # 第2步：处理有效长度（每个专家都要知道句子的真实长度）\n",
    "        if valid_lens is not None:\n",
    "            valid_lens = torch.repeat_interleave(\n",
    "                valid_lens, repeats=self.num_heads, dim=0)\n",
    "            \n",
    "        # 第3步：每个专家并行计算注意力\n",
    "        output = self.attention(queries, keys, values, valid_lens)\n",
    "        # 第4步：收集所有专家的成果并合并\n",
    "        output_concat = self.transpose_output(output)\n",
    "        return self.W_o(output_concat) # 最终融合输出"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e19a39-01d7-4646-bb17-c6590e8174c0",
   "metadata": {},
   "source": [
    "为了能够使多个头并行计算， 上面的MultiHeadAttention类将使用下面定义的两个转置函数。 具体来说，transpose_output函数反转了transpose_qkv函数的操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a4b714a-56a0-4a44-8848-341f54f36519",
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(MultiHeadAttention)  #@save\n",
    "def transpose_qkv(self, X):\n",
    "    \"\"\"Transposition for parallel computation of multiple attention heads.\"\"\"\n",
    "    # Shape of input X: (batch_size, no. of queries or key-value pairs,\n",
    "    # num_hiddens). Shape of output X: (batch_size, no. of queries or\n",
    "    # key-value pairs, num_heads, num_hiddens / num_heads)\n",
    "    X = X.reshape(X.shape[0], X.shape[1], self.num_heads, -1)\n",
    "    # Shape of output X: (batch_size, num_heads, no. of queries or key-value\n",
    "    # pairs, num_hiddens / num_heads)\n",
    "    X = X.permute(0, 2, 1, 3)\n",
    "    # Shape of output: (batch_size * num_heads, no. of queries or key-value\n",
    "    # pairs, num_hiddens / num_heads)\n",
    "    return X.reshape(-1, X.shape[2], X.shape[3])\n",
    "\n",
    "@d2l.add_to_class(MultiHeadAttention)  #@save\n",
    "def transpose_output(self, X):\n",
    "    \"\"\"Reverse the operation of transpose_qkv.\"\"\"\n",
    "    X = X.reshape(-1, self.num_heads, X.shape[1], X.shape[2])\n",
    "    X = X.permute(0, 2, 1, 3)\n",
    "    return X.reshape(X.shape[0], X.shape[1], -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3712d7ba-2b4c-42a3-a9fc-a837b4de8e8e",
   "metadata": {},
   "source": [
    "```\n",
    "transpose_qkv的作用：\n",
    "\"大向量\" → \"多个小向量\" → \"便于并行计算\"\n",
    "(2,4,100) → (2,4,5,20) → (2,5,4,20) → (10,4,20)\n",
    " 原始输入    分成5组      重排便于处理   10个并行任务"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef94aaf-ff79-4188-9e83-bd6a0b11d616",
   "metadata": {},
   "source": [
    " ```\n",
    "transpose_output的作用：\n",
    "\"多个小结果\" → \"大结果\"\n",
    "(10,4,20) → (2,5,4,20) → (2,4,5,20) → (2,4,100)\n",
    "10个结果     重新分组      恢复维度      合并输出"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea5191d-2b7c-475b-af5a-3448d80bd7e0",
   "metadata": {},
   "source": [
    "下面使用键和值相同的小例子来测试我们编写的MultiHeadAttention类。 多头注意力输出的形状是（batch_size，num_queries，num_hiddens）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9a9c061-0313-45ba-9dc1-ca79a44038f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hiddens, num_heads = 100, 5\n",
    "attention = MultiHeadAttention(num_hiddens, num_heads, 0.5)\n",
    "batch_size, num_queries, num_kvpairs = 2, 4, 6\n",
    "valid_lens = torch.tensor([3, 2])\n",
    "X = torch.ones((batch_size, num_queries, num_hiddens))\n",
    "Y = torch.ones((batch_size, num_kvpairs, num_hiddens))\n",
    "d2l.check_shape(attention(X, Y, Y, valid_lens),\n",
    "                (batch_size, num_queries, num_hiddens))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (Lee Course)",
   "language": "python",
   "name": "lee_course_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
